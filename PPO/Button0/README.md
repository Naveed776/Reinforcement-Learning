# PPO Point Button0 

Iteration 1:
The average episode length was 1000, and the average episode reward was 4.91. The training FPS was 478, with total timesteps reaching 3000, and a time elapsed of 6 seconds. The training metrics included an approximate KL divergence of 0.0016, a clip fraction of 0.5%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was -0.00495. The learning rate was set at 0.0001, with a loss of 0.00236, number of updates at 10, a policy gradient loss of -0.000396, a standard deviation of 0.999, and a value loss of 0.00276.

Iteration 2:
In this iteration, the average episode length remained at 1000, and the average episode reward increased slightly to 5.12. The training FPS improved to 519, with total timesteps of 6000 and a time elapsed of 12 seconds. The training metrics showed an approximate KL divergence of 0.0025, a clip fraction of 1.5%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance improved to 0.00269. The learning rate remained at 0.0001, with a loss of -0.00195, number of updates at 20, a policy gradient loss of -0.00168, a standard deviation of 0.997, and a value loss of 0.00308.

Iteration 3:
The average episode length was 1000, and the average episode reward was 6.51. The training FPS was 529, with total timesteps reaching 9000, and a time elapsed of 17 seconds. The training metrics included an approximate KL divergence of 0.0043, a clip fraction of 3.6%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.22. The learning rate was 0.0001, with a loss of -0.00351, number of updates at 30, a policy gradient loss of -0.00216, a standard deviation of 0.997, and a value loss of 0.00756.

Iteration 4:
For this iteration, the average episode length remained at 1000, and the average episode reward was 8.17. The training FPS increased to 533, with total timesteps of 12000 and a time elapsed of 23 seconds. The training metrics showed an approximate KL divergence of 0.0039, a clip fraction of 2.5%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.336. The learning rate was 0.0001, with a loss of 0.00358, number of updates at 40, a policy gradient loss of -0.00307, a standard deviation of 0.996, and a value loss of 0.00859.

Iteration 5:
The average episode length remained at 1000, and the average episode reward was 9.3. The training FPS was 536, with total timesteps reaching 15000, and a time elapsed of 28 seconds. The training metrics included an approximate KL divergence of 0.003, a clip fraction of 3.1%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance improved to 0.456. The learning rate was 0.0001, with a loss of -0.00282, number of updates at 50, a policy gradient loss of -0.00285, a standard deviation of 0.996, and a value loss of 0.00983.

Iteration 6:
In this iteration, the average episode length remained at 1000, and the average episode reward increased to 9.91. The training FPS was 538, with total timesteps of 18000 and a time elapsed of 33 seconds. The training metrics showed an approximate KL divergence of 0.0038, a clip fraction of 3.0%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.392. The learning rate remained at 0.0001, with a loss of 0.0087, number of updates at 60, a policy gradient loss of -0.00236, a standard deviation of 0.995, and a value loss of 0.0137.

Iteration 7:
The average episode length was 1000, and the average episode reward was 10.0. The training FPS was 539, with total timesteps reaching 21000, and a time elapsed of 38 seconds. The training metrics included an approximate KL divergence of 0.0035, a clip fraction of 2.7%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.448. The learning rate was 0.0001, with a loss of -0.00336, number of updates at 70, a policy gradient loss of -0.00212, a standard deviation of 0.995, and a value loss of 0.00782.

Iteration 8:
For this iteration, the average episode length remained at 1000, and the average episode reward was 10.1. The training FPS was 540, with total timesteps of 24000 and a time elapsed of 43 seconds. The training metrics showed an approximate KL divergence of 0.0026, a clip fraction of 2.3%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.294. The learning rate was 0.0001, with a loss of -0.00572, number of updates at 80, a policy gradient loss of -0.00251, a standard deviation of 0.995, and a value loss of 0.00937.

Iteration 9:
The average episode length remained at 1000, and the average episode reward increased to 10.5. The training FPS was 541, with total timesteps reaching 27000, and a time elapsed of 48 seconds. The training metrics included an approximate KL divergence of 0.0041, a clip fraction of 3.6%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.438. The learning rate was 0.0001, with a loss of -0.00382, number of updates at 90, a policy gradient loss of -0.00311, a standard deviation of 0.994, and a value loss of 0.00786.

Iteration 10:
In this iteration, the average episode length remained at 1000, and the average episode reward was 10.9. The training FPS was 542, with total timesteps of 30000 and a time elapsed of 53 seconds. The training metrics showed an approximate KL divergence of 0.0025, a clip fraction of 1.8%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.293. The learning rate remained at 0.0001, with a loss of 0.0061, number of updates at 100, a policy gradient loss of -0.00242, a standard deviation of 0.995, and a value loss of 0.0103.

Iteration 11:
The average episode length was 1000, and the average episode reward was 11.0. The training FPS was 543, with total timesteps reaching 33000, and a time elapsed of 58 seconds. The training metrics included an approximate KL divergence of 0.0041, a clip fraction of 3.5%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance improved to 0.506. The learning rate was 0.0001, with a loss of -0.0076, number of updates at 110, a policy gradient loss of -0.00291, a standard deviation of 0.994, and a value loss of 0.0136.

Iteration 12:
For this iteration, the average episode length remained at 1000, and the average episode reward was 11.1. The training FPS was 543, with total timesteps of 36000 and a time elapsed of 63 seconds. The training metrics showed an approximate KL divergence of 0.0054, a clip fraction of 6.6%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.415. The learning rate was 0.0001, with a loss of -0.0043, number of updates at 110, a policy gradient loss of -0.00311, a standard deviation of 0.995, and a value loss of 0.0087.

Iteration 13:
The average episode length remained at 1000, and the average episode reward increased slightly to 11.2. The training FPS was 539, with total timesteps reaching 39000, and a time elapsed of 69 seconds. The training metrics included an approximate KL divergence of 0.0021, a clip fraction of 1.8%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.567. The learning rate was 0.0001, with a loss of -0.0042, number of updates at 120, a policy gradient loss of -0.00321, a standard deviation of 0.995, and a value loss of 0.0081.

Iteration 14:
In this iteration, the average episode length remained at 1000, and the average episode reward was 11.3. The training FPS was 541, with total timesteps of 42000 and a time elapsed of 74 seconds. The training metrics showed an approximate KL divergence of 0.0043, a clip fraction of 3.1%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance improved to 0.468. The learning rate remained at 0.0001, with a loss of -0.0039, number of updates at 130, a policy gradient loss of -0.00331, a standard deviation of 0.994, and a value loss of 0.0093.

Iteration 15:
The average episode length was 1000, and the average episode reward increased to 11.4. The training FPS was 544, with total timesteps reaching 45000, and a time elapsed of 79 seconds. The training metrics included an approximate KL divergence of 0.0036, a clip fraction of 2.8%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.587. The learning rate was 0.0001, with a loss of -0.00281, number of updates at 140, a policy gradient loss of -0.00361, a standard deviation of 0.994, and a value loss of 0.0072.

Iteration 16:
For this iteration, the average episode length remained at 1000, and the average episode reward was 11.5. The training FPS was 546, with total timesteps of 48000 and a time elapsed of 84 seconds. The training metrics showed an approximate KL divergence of 0.005, a clip fraction of 4.4%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.426. The learning rate was 0.0001, with a loss of -0.0032, number of updates at 150, a policy gradient loss of -0.00412, a standard deviation of 0.994, and a value loss of 0.0085.

Iteration 17:
The average episode length remained at 1000, and the average episode reward increased to 11.6. The training FPS was 546, with total timesteps reaching 51000, and a time elapsed of 89 seconds. The training metrics included an approximate KL divergence of 0.0042, a clip fraction of 3.2%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.54. The learning rate was 0.0001, with a loss of -0.0029, number of updates at 160, a policy gradient loss of -0.00351, a standard deviation of 0.993, and a value loss of 0.0074.

Iteration 18:
In this iteration, the average episode length remained at 1000, and the average episode reward was 11.7. The training FPS was 546, with total timesteps of 54000 and a time elapsed of 94 seconds. The training metrics showed an approximate KL divergence of 0.0033, a clip fraction of 2.8%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.617. The learning rate remained at 0.0001, with a loss of -0.0025, number of updates at 170, a policy gradient loss of -0.0031, a standard deviation of 0.993, and a value loss of 0.0067.

Iteration 19:
The average episode length was 1000, and the average episode reward was 12.1. The training FPS was 547, with total timesteps reaching 57000, and a time elapsed of 99 seconds. The training metrics included an approximate KL divergence of 0.0027, a clip fraction of 2.4%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.671. The learning rate was 0.0001, with a loss of -0.0026, number of updates at 180, a policy gradient loss of -0.0027, a standard deviation of 0.993, and a value loss of 0.007.

Iteration 20:
For this iteration, the average episode length remained at 1000, and the average episode reward increased to 12.2. The training FPS was 547, with total timesteps of 60000 and a time elapsed of 104 seconds. The training metrics showed an approximate KL divergence of 0.0038, a clip fraction of 3.2%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.496. The learning rate was 0.0001, with a loss of -0.0026, number of updates at 190, a policy gradient loss of -0.0028, a standard deviation of 0.992, and a value loss of 0.0063.

Iteration 21:
The average episode length remained at 1000, and the average episode reward was 12.3. The training FPS was 548, with total timesteps reaching 63000, and a time elapsed of 109 seconds. The training metrics included an approximate KL divergence of 0.0045, a clip fraction of 3.9%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.509. The learning rate was 0.0001, with a loss of -0.0025, number of updates at 200, a policy gradient loss of -0.0025, a standard deviation of 0.992, and a value loss of 0.0059.

Iteration 22:
In this iteration, the average episode length remained at 1000, and the average episode reward increased to 12.6. The training FPS was 548, with total timesteps of 66000 and a time elapsed of 114 seconds. The training metrics showed an approximate KL divergence of 0.0029, a clip fraction of 2.5%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.592. The learning rate remained at 0.0001, with a loss of -0.0025, number of updates at 210, a policy gradient loss of -0.0026, a standard deviation of 0.992, and a value loss of 0.006.

Iteration 23:
The average episode length was 1000, and the average episode reward was 13.4. The training FPS was 548, with total timesteps reaching 69000, and a time elapsed of 119 seconds. The training metrics included an approximate KL divergence of 0.0044, a clip fraction of 3.8%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.498. The learning rate was 0.0001, with a loss of -0.00249, number of updates at 220, a policy gradient loss of -0.00249, a standard deviation of 0.991, and a value loss of 0.0116.

Iteration 24:
For this iteration, the average episode length remained at 1000, and the average episode reward was 13.9. The training FPS was 548, with total timesteps of 72000 and a time elapsed of 128 seconds. The training metrics showed an approximate KL divergence of 0.0029, a clip fraction of 2.76%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.68. The learning rate remained at 0.0001, with a loss of -0.00272, number of updates at 230, a policy gradient loss of -0.00169, a standard deviation of 0.99, and a value loss of 0.00576.

Iteration 25:
In this final iteration, the average episode length remained at 1000, and the average episode reward increased to 14.4. The training FPS was 548, with total timesteps reaching 75000, and a time elapsed of 133 seconds. The training metrics included an approximate KL divergence of 0.0034, a clip fraction of 2.52%, and a clip range of 0.2. The entropy loss was -2.82, and the explained variance was 0.72. The learning rate was 0.0001, with a loss of -0.00282, number of updates at 240, a policy gradient loss of -0.00179, a standard deviation of 0.989, and a value loss of 0.00682.


[Screencast from 06-30-2024 07:56:43 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/89fa6501-c9c9-4c9f-8eac-2dd884b816bc)



## summary of the training process:

Iterations 1-5: The average episode length was consistently 1000, with the average episode reward gradually increasing from 9.1 to 10.8. The training FPS ranged between 533 and 540. Key metrics such as the approximate KL divergence (0.0018 to 0.0056), entropy loss (-2.83 to -2.82), explained variance (0.224 to 0.522), and learning rate (0.0001) remained relatively stable, while the policy gradient loss and value loss showed slight variations.

Iterations 6-10: The average episode reward continued to improve, reaching 11. The training FPS ranged from 541 to 543. The approximate KL divergence fluctuated between 0.0026 and 0.0045, with the entropy loss remaining at -2.82. The explained variance varied from 0.34 to 0.544, and the learning rate stayed constant at 0.0001. The policy gradient loss and value loss exhibited minor changes.

Iterations 11-15: The average episode reward increased from 11 to 11.4, with the training FPS ranging from 544 to 546. The approximate KL divergence remained between 0.003 and 0.005, and the entropy loss stayed at -2.82. The explained variance ranged from 0.426 to 0.587. The learning rate was constant at 0.0001, with slight variations in the policy gradient loss and value loss.

Iterations 16-20: The average episode reward continued to rise, reaching 12.2. The training FPS ranged from 546 to 547. The approximate KL divergence varied from 0.0027 to 0.005, with the entropy loss remaining at -2.82. The explained variance fluctuated between 0.496 and 0.671. The learning rate stayed at 0.0001, with minor changes in the policy gradient loss and value loss


Iterations 21-25: The average episode reward increased from 12.3 to 14.4. The training FPS was consistent at 548. The approximate KL divergence ranged from 0.0029 to 0.0045, and the entropy loss remained at -2.82. The explained variance improved, reaching up to 0.72. The learning rate was constant at 0.0001, with minor fluctuations in the policy gradient loss and value loss.

Overall, the training process showed a consistent improvement in average episode reward, with key metrics such as KL divergence, entropy loss, explained variance, and learning rate remaining relatively stable. The training FPS was consistent, indicating a steady training pace.

## Certainly! Here is a summarized version of the episode returns data:

The agent's performance across 25 episodes shows varying returns:

Episode returns range from 18.272 to 40.984.
The average return is approximately 28.26.
Episode costs are consistently reported as 0.000, indicating no incurred costs.
This data suggests the agent achieves moderate to high returns consistently, with no associated costs observed during these episodes.
![ppopointbuttton0](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/cd589faa-ae4e-479f-97d1-7a9cfd402a85)

