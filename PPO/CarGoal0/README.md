# CarGoal0

At iteration 1, the average episode length was 1000, with an average reward of 8.15. The system operated at 485 frames per second (fps), with a total of 3000 timesteps, taking 6 seconds to complete.

By iteration 2, the average episode length remained at 1000, but the average reward increased to 8.29. The system's performance improved slightly to 487 fps, with a total of 6000 timesteps over 12 seconds. Training statistics included an approximate KL divergence of 0.0059, a clip fraction of 0.0612, an entropy loss of -2.84, an explained variance of -6.94, a learning rate of 0.0001, a loss of 0.00012, a policy gradient loss of -0.00641, a standard deviation of 1, and a value loss of 0.0182.

At iteration 3, the average episode length was still 1000, and the average reward increased to 8.56. The system achieved 504 fps, with a total of 9000 timesteps in 17 seconds. Training statistics showed an approximate KL divergence of 0.0064, a clip fraction of 0.067, an entropy loss of -2.84, an explained variance of 0.0876, a learning rate of 0.0001, a loss of -0.00896, a policy gradient loss of -0.00738, a standard deviation of 0.999, and a value loss of 0.00509.

By iteration 4, the average episode length was again 1000, and the average reward increased to 8.92. The system ran at 500 fps, with 12000 total timesteps over 23 seconds. Training statistics included an approximate KL divergence of 0.0093, a clip fraction of 0.11, an entropy loss of -2.83, an explained variance of 0.139, a learning rate of 0.0001, a loss of -0.0105, a policy gradient loss of -0.0129, a standard deviation of 0.996, and a value loss of 0.0052.

At iteration 5, the average episode length was 1000, and the average reward increased to 9.07. The system's fps was 505, with 15000 total timesteps over 29 seconds. Training statistics showed an approximate KL divergence of 0.0105, a clip fraction of 0.147, an entropy loss of -2.82, an explained variance of -0.349, a learning rate of 0.0001, a loss of -0.0278, a policy gradient loss of -0.0146, a standard deviation of 0.988, and a value loss of 0.00579.

At iteration 6, the average episode length was 1000, and the average reward increased to 9.3. The system operated at 505 fps, with a total of 18000 timesteps in 35 seconds. Training statistics included an approximate KL divergence of 0.0049, a clip fraction of 0.0216, an entropy loss of -2.81, an explained variance of -0.325, a learning rate of 0.0001, a loss of -0.00111, a policy gradient loss of -0.00525, a standard deviation of 0.986, and a value loss of 0.00863.

By iteration 7, the average episode length was 1000, and the average reward was 9.32. The system achieved 510 fps, with a total of 21000 timesteps over 41 seconds. Training statistics showed an approximate KL divergence of 0.0054, a clip fraction of 0.0236, an entropy loss of -2.81, an explained variance of -0.114, a learning rate of 0.0001, a loss of 0.00482, a policy gradient loss of -0.00488, a standard deviation of 0.984, and a value loss of 0.00925.

At iteration 8, the average episode length was 1000, and the average reward was 9.31. The system's fps was 514, with 24000 total timesteps over 46 seconds. Training statistics included an approximate KL divergence of 0.0058, a clip fraction of 0.0523, an entropy loss of -2.8, an explained variance of -0.166, a learning rate of 0.0001, a loss of -0.00726, a policy gradient loss of -0.00656, a standard deviation of 0.983, and a value loss of 0.00727.

By iteration 9, the average episode length was 1000, and the average reward increased to 9.36. The system achieved 516 fps, with a total of 27000 timesteps over 52 seconds. Training statistics showed an approximate KL divergence of 0.0077, a clip fraction of 0.0994, an entropy loss of -2.8, an explained variance of -0.22, a learning rate of 0.0001, a loss of -0.004, a policy gradient loss of -0.00924, a standard deviation of 0.983, and a value loss of 0.00576.

At iteration 10, the average episode length was 1000, and the average reward increased to 9.47. The system operated at 517 fps, with a total of 30000 timesteps over 57 seconds. Training statistics included an approximate KL divergence of 0.0069, a clip fraction of 0.0951, an entropy loss of -2.8, an explained variance of 0.0006, a learning rate of 0.0001, a loss of -0.0204, a policy gradient loss of -0.0108, a standard deviation of 0.98, and a value loss of 0.00637.

By iteration 11, the average episode length was 1000, and the average reward increased to 9.52. The system's fps was 519, with 33000 total timesteps over 63 seconds. Training statistics showed an approximate KL divergence of 0.0049, a clip fraction of 0.0378, an entropy loss of -2.8, an explained variance of -0.0487, a learning rate of 0.0001, a loss of -0.00801, a policy gradient loss of -0.00839, a standard deviation of 0.979, and a value loss of 0.0065.

At iteration 12, the average episode length was 1000, and the average reward increased to 9.63. The system achieved 521 fps, with a total of 36000 timesteps over 68 seconds. Training statistics included an approximate KL divergence of 0.0084, a clip fraction of 0.0739, an entropy loss of -2.79, an explained variance of -0.0662, a learning rate of 0.0001, a loss of -0.0146, a policy gradient loss of -0.009, a standard deviation of 0.978, and a value loss of 0.00867.

By iteration 13, the average episode length was 1000, and the average reward increased to 9.71. The system operated at 522 fps, with a total of 39000 timesteps over 74 seconds. Training statistics showed an approximate KL divergence of 0.0087, a clip fraction of 0.0911, an entropy loss of -2.79, an explained variance of -0.0658, a learning rate of 0.0001, a loss of -0.0052, a policy gradient loss of -0.00788, a standard deviation of 0.975, and a value loss of 0.0101.

At iteration 14, the average episode length was 1000, and the average reward increased to 9.76. The system achieved 524 fps, with a total of 42000 timesteps over 80 seconds. Training statistics included an approximate KL divergence of 0.0081, a clip fraction of 0.0656, an entropy loss of -2.78, an explained variance of 0.0261, a learning rate of 0.0001, a loss of -0.016, a policy gradient loss of -0.0087, a standard deviation of 0.972, and a value loss of 0.00895.
By iteration 15, the average episode length was 1000, and the average reward increased to 9.82. The system's fps was 524, with 45000 total timesteps over 85 seconds. Training statistics showed an approximate KL divergence of 0.0054, a clip fraction of 0.0396, an entropy loss of -2.78, an explained variance of 0.247, a learning rate of 0.0001, a loss of -0.00958, a policy gradient loss of -0.0052, a standard deviation of 0.97, and a value loss of 0.00932.

At iteration 16, the average episode length was 1000, and the average reward was 9.93. The system achieved 525 fps, with a total of 48000 timesteps over 91 seconds. Training statistics included an approximate KL divergence of 0.0047, a clip fraction of 0.0286, an entropy loss of -2.77, an explained variance of -0.181, a learning rate of 0.0001, a loss of 0.0111, a policy gradient loss of -0.00353, a standard deviation of 0.969, and a value loss of 0.0105.

By iteration 17, the average episode length was 1000, and the average reward increased to 10. The system operated at 525 fps, with a total of 51000 timesteps over 97 seconds. Training statistics showed an approximate KL divergence of 0.0046, a clip fraction of 0.0203, an entropy loss of -2.77, an explained variance of 0.0443, a learning rate of 0.0001, a loss of -0.00714, a policy gradient loss of -0.00387, a standard deviation of 0.967, and a value loss of 0.0102.

At iteration 18, the average episode length was 1000, and the average reward increased to 10. The system achieved 528 fps, with a total of 54000 timesteps over 102 seconds. Training statistics included an approximate KL divergence of 0.0049, a clip fraction of 0.0318, an entropy loss of -2.76, an explained variance of 0.042, a learning rate of 0.0001, a loss of -0.0127, a policy gradient loss of -0.00462, a standard deviation of 0.965, and a value loss of 0.00906.

By iteration 19, the average episode length was 1000, and the average reward was 10. The system's fps was 527, with 57000 total timesteps over 108 seconds. Training statistics showed an approximate KL divergence of 0.0054, a clip fraction of 0.0485, an entropy loss of -2.76, an explained variance of -0.018, a learning rate of 0.0001, a loss of -0.00205, a policy gradient loss of -0.00532, a standard deviation of 0.962, and a value loss of 0.0101.

At iteration 20, the average episode length was 1000, and the average reward increased to 10. The system achieved 528 fps, with a total of 60000 timesteps over 113 seconds. Training statistics included an approximate KL divergence of 0.0047, a clip fraction of 0.0225, an entropy loss of -2.76, an explained variance of -0.226, a learning rate of 0.0001, a loss of -0.00244, a policy gradient loss of -0.00423, a standard deviation of 0.961, and a value loss of 0.00973.

## Summarized version 
    • Iterations: 20
    • Average Episode Length: Consistently 1000 across all iterations.
    • Average Reward: Increased from 8.15 to 10.0 over the course of training.
    • System Performance: Improved from 485 fps to 528 fps.
    • Training Statistics:
        ◦ KL Divergence: Varied between 0.0046 and 0.0105.
        ◦ Clip Fraction: Ranged from 0.0203 to 0.147.
        ◦ Entropy Loss: Decreased from -2.84 to -2.76.
        ◦ Explained Variance: Varied, peaking at 0.247 but generally fluctuating.
        ◦ Policy Gradient Loss: Mostly negative, indicating successful learning.
        ◦ Standard Deviation: Slightly decreased from 1.000 to around 0.961.
        ◦ Value Loss: Fluctuated, showing minor changes around 0.010.
Overall, the model showed a steady improvement in average reward and system performance, with stable training statistics.

[Screencast from 06-30-2024 09:12:11 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/a3bc153a-fbf7-4dea-b5ba-15cdb9208ae2)




## Summary of Episode
In Episode 1, the return was 14.872 with an episode cost of 0.000. Episode 2 saw a return of 17.016, maintaining the episode cost at 0.000. In Episode 3, the return dropped to 13.670, still with no episode cost. Episode 4 achieved a return of 15.753, and Episode 5 saw a slight increase to 16.770, both without any episode cost.
Episode 6 experienced a decrease in return to 11.715, followed by a rise in Episode 7 to 17.207. Episode 8 had a return of 17.165, and Episode 9 slightly increased to 17.381. Episode 10 experienced a return drop to 13.451, but Episode 11 saw an improvement to 15.257. Episode 12 had a return of 15.637, and Episode 13 further increased to 16.810.
In Episode 14, the return was 14.972, while Episode 15 had a return of 13.923. Episode 16 recorded a return of 14.769, followed by a significant rise in Episode 17 to 19.595. Episode 18 had a return of 16.599, and Episode 19 saw a slight decrease to 14.758. Episode 20 had a return of 16.330, and Episode 21 recorded a return of 16.841.
Episode 22 experienced a decrease to 13.524, but Episode 23 saw an improvement to 15.075. Episode 24 recorded a return of 14.856, and Episode 25 saw a drop to 11.673. Throughout all episodes, the episode cost remained consistently at 0.000.
Certainly! Here are some conclusions based on the data provided:
    1. Return Variation: The returns across episodes varied between 11.673 and 19.595. This variability indicates different levels of success or performance in the task or environment being simulated or trained.
    2. Consistent Episode Cost: Throughout all episodes, the episode cost remained at 0.000. This suggests that the cost associated with each episode, possibly related to computational resources or time, was consistently managed or minimized.
    3. Training Stability: The training process appears stable overall, with fluctuations in return but without any apparent upward or downward trend over the 25 episodes.
    4. Episode-by-Episode Analysis: While some episodes showed higher returns (e.g., Episode 17 with 19.595), others had lower returns (e.g., Episode 25 with 11.673). This variability could reflect different stages of learning or exploration within the training process.
    5. Performance Metrics: Further analysis beyond returns, such as exploration vs. exploitation balance, convergence metrics, or learning curve analysis, would provide deeper insights into the effectiveness and efficiency of the training process.
Overall, the data suggests ongoing training with varied returns but stable episode costs, indicating a potentially effective learning process with room for further optimization or exploration.
![cargoal0](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/4248fd33-bdf4-4160-bc87-30b845552d9f)


