
# PPO Point Goal0 
1. The first iteration, the episode length mean is 1000 with an episode reward mean of 9.23. The frames per second (fps) is 554, and the total time elapsed is 5 seconds with 3000 total timesteps.

2. At the second iteration, the episode length mean remains at 1000, and the episode reward mean increases slightly to 9.49. The fps drops to 519, with the total time elapsed reaching 11 seconds and 6000 total timesteps. The approximate KL divergence is 0.0052942894, the clip fraction is 0.0477, the clip range is 0.2, and the entropy loss is -2.83. The explained variance is -8.19, the learning rate is 0.0001, the loss is -8.73e-05, and there have been 10 updates. The policy gradient loss is -0.00515, the standard deviation is 0.997, and the value loss is 0.00328.

3. By the third iteration, the episode length mean stays at 1000, and the episode reward mean slightly decreases to 9.46. The fps is 537, with the total time elapsed at 16 seconds and 9000 total timesteps. The approximate KL divergence is 0.0045102676, the clip fraction is 0.0426, the clip range is 0.2, and the entropy loss is -2.83. The explained variance is 0.105, the learning rate is 0.0001, the loss is -0.00416, and there have been 20 updates. The policy gradient loss is -0.00392, the standard deviation is 0.994, and the value loss is 0.00255.

4. For the fourth iteration, the episode length mean is still 1000, with an episode reward mean of 9.4. The fps increases to 553, with a total time elapsed of 21 seconds and 12000 total timesteps. The approximate KL divergence is 0.003940412, the clip fraction is 0.0239, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is -0.152, the learning rate is 0.0001, the loss is -0.00251, and there have been 30 updates. The policy gradient loss is -0.00285, the standard deviation is 0.992, and the value loss is 0.00655.

5. By the fifth iteration, the episode length mean remains at 1000, and the episode reward mean drops slightly to 9.26. The fps is 559, with a total time elapsed of 26 seconds and 15000 total timesteps. The approximate KL divergence is 0.005214433, the clip fraction is 0.0335, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is 0.0141, the learning rate is 0.0001, the loss is -0.00232, and there have been 40 updates. The policy gradient loss is -0.0037, the standard deviation is 0.99, and the value loss is 0.00658.

6. At the sixth iteration, the episode length mean stays at 1000, and the episode reward mean is 9.26. The fps is 563, with a total time elapsed of 31 seconds and 18000 total timesteps. The approximate KL divergence is 0.0066863215, the clip fraction is 0.0692, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is 0.178, the learning rate is 0.0001, the loss is -0.000864, and there have been 50 updates. The policy gradient loss is -0.0059, the standard deviation is 0.991, and the value loss is 0.00378.

7. By the seventh iteration, the episode length mean remains 1000, and the episode reward mean increases to 9.43. The fps is 567, with a total time elapsed of 37 seconds and 21000 total timesteps. The approximate KL divergence is 0.0053134738, the clip fraction is 0.0745, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is 0.172, the learning rate is 0.0001, the loss is -0.00858, and there have been 60 updates. The policy gradient loss is -0.00659, the standard deviation is 0.99, and the value loss is 0.00452.

8. At the eighth iteration, the episode length mean is still 1000, and the episode reward mean increases to 9.64. The fps is 570, with a total time elapsed of 42 seconds and 24000 total timesteps. The approximate KL divergence is 0.0084912125, the clip fraction is 0.081, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is 0.084, the learning rate is 0.0001, the loss is -0.0133, and there have been 70 updates. The policy gradient loss is -0.00624, the standard deviation is 0.989, and the value loss is 0.00802.

9. By the ninth iteration, the episode length mean remains at 1000, and the episode reward mean increases to 9.87. The fps is 565, with a total time elapsed of 47 seconds and 27000 total timesteps. The approximate KL divergence is 0.007162906, the clip fraction is 0.0478, the clip range is 0.2, and the entropy loss is -2.82. The explained variance is 0.164, the learning rate is 0.0001, the loss is 0.00511, and there have been 80 updates. The policy gradient loss is -0.00364, the standard deviation is 0.989, and the value loss is 0.0107.

10. At the tenth iteration, the episode length mean is still 1000, and the episode reward mean increases to 10.1. The fps is 562, with a total time elapsed of 53 seconds and 30000 total timesteps. The approximate KL divergence is 0.0060064285, the clip fraction is 0.0625, the clip range is 0.2, and the entropy loss is -2.81. The explained variance is 0.283, the learning rate is 0.0001, the loss is 0.000392, and there have been 90 updates. The policy gradient loss is -0.00439, the standard deviation is 0.986, and the value loss is 0.0129.

11. By the eleventh iteration, the episode length mean remains 1000, and the episode reward mean increases to 10.2. The fps is 568, with a total time elapsed of 58 seconds and 33000 total timesteps. The approximate KL divergence is 0.0042069573, the clip fraction is 0.0445, the clip range is 0.2, and the entropy loss is -2.81. The explained variance is 0.435, the learning rate is 0.0001, the loss is 0.000798, and there have been 100 updates. The policy gradient loss is -0.00353, the standard deviation is 0.982, and the value loss is 0.0124.

12. At the twelfth iteration, the episode length mean is still 1000, and the episode reward mean increases to 10.4. The fps is 573, with a total time elapsed of 62 seconds and 36000 total timesteps. The approximate KL divergence is 0.0045859455, the clip fraction is 0.0272, the clip range is 0.2, and the entropy loss is -2.8. The explained variance is 0.442, the learning rate is 0.0001, the loss is 0.00151, and there have been 110 updates. The policy gradient loss is -0.00268, the standard deviation is 0.98, and the value loss is 0.0152.

13. By the thirteenth iteration, the episode length mean remains at 1000, and the episode reward mean increases to 10.6. The fps is 573, with a total time elapsed of 68 seconds and 39000 total timesteps. The approximate KL divergence is 0.006130088, the clip fraction is 0.0661, the clip range is 0.2, and the entropy loss is -2.8. The explained variance is 0.664, the learning rate is 0.0001, the loss is -0.00481, and there have been 120 updates. The policy gradient loss is -0.00459, the standard deviation is 0.98, and the value loss is 0.0139.

14. At the fourteenth iteration, the episode length mean is still 1000, and the episode reward mean increases to 10.9. The fps is 573, with a total time elapsed of 73 seconds and 42000 total timesteps. The approximate KL divergence is 0.0055086225, the clip fraction is 0.0538, the clip range is 0.2, and the entropy loss is -2.79. The explained variance is 0.614, the learning rate is 0.0001, the loss is 0.000422, and there have been 130 updates. The policy gradient loss is -0.00324, the standard deviation is 0.98, and the value loss is 0.0171.

15. By the fifteenth iteration, the episode length mean remains 1000, and the episode reward mean increases to 11.1. The fps is 574, with a total time elapsed of 79 seconds and 45000 total timesteps. The approximate KL divergence is 0.0047357973, the clip fraction is 0.0491, the clip range is 0.2, and the entropy loss is -2.79. The explained variance is 0.684, the learning rate is 0.0001, the loss is 0.00076, and there have been 140 updates. The policy gradient loss is -0.00309, the standard deviation is 0.98, and the value loss is 0.0174.

16. At the sixteenth iteration, the episode length mean is still 1000, and the episode reward mean increases to 11.2. The fps is 574, with a total time elapsed of 83 seconds and 48000 total timesteps. The approximate KL divergence is 0.0045445296, the clip fraction is 0.0396, the clip range is 0.2, and the entropy loss is -2.79. The explained variance is 0.786, the learning rate is 0.0001, the loss is 0.0013, and there have been 150 updates. The policy gradient loss is -0.00311, the standard deviation is 0.98, and the value loss is 0.012.

17. By the seventeenth iteration, the episode length mean remains 1000, and the episode reward mean increases to 11.3. The fps is 574, with a total time elapsed of 89 seconds and 51000 total timesteps. The approximate KL divergence is 0.006056271, the clip fraction is 0.0716, the clip range is 0.2, and the entropy loss is -2.79. The explained variance is 0.673, the learning rate is 0.0001, the loss is -0.000207, and there have been 160 updates. The policy gradient loss is -0.00488, the standard deviation is 0.98, and the value loss is 0.0103.

18. At the eighteenth iteration, the episode length mean remains 1000, and the episode reward mean increases to 11.4. The fps is 574, with a total time elapsed of 94 seconds and 54000 total timesteps. The approximate KL divergence is 0.0045971093, the clip fraction is 0.0386, the clip range is 0.2, and the entropy loss is -2.78. The explained variance is 0.694, the learning rate is 0.0001, the loss is 0.0011, and there have been 170 updates. The policy gradient loss is -0.00343, the standard deviation is 0.98, and the value loss is 0.0162.

19. By the nineteenth iteration, the episode length mean remains at 1000, and the episode reward mean increases to 11.6. The fps is 573, with a total time elapsed of 99 seconds and 57000 total timesteps. The approximate KL divergence is 0.0067282953, the clip fraction is 0.0714, the clip range is 0.2, and the entropy loss is -2.78. The explained variance is 0.732, the learning rate is 0.0001, the loss is -0.00143, and there have been 180 updates. The policy gradient loss is -0.00499, the standard deviation is 0.98, and the value loss is 0.0122.

20. At the twentieth iteration, the episode length mean is still 1000, and the episode reward mean increases to 11.8. The fps is 573, with a total time elapsed of 105 seconds and 60000 total timesteps. The approximate KL divergence is 0.0048239266, the clip fraction is 0.0453, the clip range is 0.2, and the entropy loss is -2.78. The explained variance is 0.768, the learning rate is 0.0001, the loss is -0.00138, and there have been 190 updates. The policy gradient loss is -0.0031, the standard deviation is 0.98, and the value loss is 0.0137


[PPoGoal0.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/dd9fcf5f-65a4-4ed2-87fc-9a72fe435a22)


# Summary of Iterations
## Iteration 1-5:

Episode Length Mean: Remains constant at 1000.
Episode Reward Mean: Starts at 9.23 and increases slightly to 9.26 by iteration 5.
FPS: Decreases from 554 to 519 by iteration 2, then increases to 559 by iteration 5.
Total Time Elapsed: Increases from 5 seconds to 26 seconds.
Total Timesteps: Increases from 3000 to 15000.
Approximate KL Divergence: Fluctuates slightly, ending at 0.0052.
Clip Fraction: Remains low, ranging between 0.0239 to 0.0477.
Entropy Loss: Consistently around -2.82.
Explained Variance: Varies, ending at 0.0141.
Learning Rate: Constant at 0.0001.
Loss: Negative initially, slightly increases to -0.00232.
Policy Gradient Loss: Small negative values.
Standard Deviation: Slightly decreases from 0.997 to 0.99.
Value Loss: Minor fluctuations, ending at 0.00658.

## Iteration 6-10:

Episode Length Mean: Constant at 1000.
Episode Reward Mean: Increases from 9.26 to 10.1 by iteration 10.
FPS: Increases from 563 to 562.
Total Time Elapsed: Increases from 31 seconds to 53 seconds.
Total Timesteps: Increases from 18000 to 30000.
Approximate KL Divergence: Small fluctuations, ending at 0.006.
Clip Fraction: Rises to 0.081 by iteration 8, then stabilizes around 0.0625.
Entropy Loss: Remains consistent around -2.82.
Explained Variance: Improves, reaching 0.283 by iteration 10.
Learning Rate: Constant at 0.0001.
Loss: Moves to a small positive value of 0.000392.
Policy Gradient Loss: Slightly negative values.
Standard Deviation: Continues decreasing to 0.986.
Value Loss: Increases to 0.0129.
## Iteration 11-15:

Episode Length Mean: Constant at 1000.
Episode Reward Mean: Increases from 10.2 to 11.1.
FPS: Increases slightly from 568 to 574.
Total Time Elapsed: Increases from 58 seconds to 79 seconds.
Total Timesteps: Increases from 33000 to 45000.
Approximate KL Divergence: Fluctuates, reaching 0.0047.
Clip Fraction: Ranges from 0.0272 to 0.0716.
Entropy Loss: Stable around -2.8.
Explained Variance: Improves, reaching 0.684 by iteration 15.
Learning Rate: Constant at 0.0001.
Loss: Slightly positive, around 0.00076.
Policy Gradient Loss: Consistently negative but minor.
Standard Deviation: Stabilizes around 0.98.
Value Loss: Continues minor fluctuations, ending at 0.0174.

## Iteration 16-20:

Episode Length Mean: Constant at 1000.
Episode Reward Mean: Increases from 11.2 to 11.8.
FPS: Stabilizes around 574.
Total Time Elapsed: Increases from 83 seconds to 105 seconds.
Total Timesteps: Increases from 48000 to 60000.
Approximate KL Divergence: Ends at 0.0048.
Clip Fraction: Varies, ending at 0.0453.
Entropy Loss: Around -2.78.
Explained Variance: Continues improving, reaching 0.768 by iteration 20.
Learning Rate: Constant at 0.0001.
Loss: Slightly negative at -0.00138.
Policy Gradient Loss: Minor negative values.
Standard Deviation: Stabilizes around 0.98.
Value Loss: Ends at 0.0137.

# Summary of Episodes' Return and Cost
The episode returns and costs provided can be analyzed to determine the average return, variability, and other statistics. Hereâ€™s a summary of the data for the 25 episodes:

The data represents the outcomes of multiple episodes in a simulation or reinforcement learning scenario, where each episode yields a return (reward) and has no associated cost.

Return (Reward):

The returns vary between episodes, ranging from a minimum of 19.060 to a maximum of 29.643.
The average return across all episodes appears to be moderately high, suggesting consistent or improving performance.
Episode Cost:

Each episode lists a cost of 0, indicating no additional expenses or penalties incurred during the simulations.
Observations:

The returns fluctuate around a certain mean value, indicating stable performance with occasional variations.
Episodes such as the 14th and 25th exhibit lower returns compared to others, which might indicate challenges or specific conditions in those episodes.
Overall, the returns suggest a successful performance in achieving the simulation's objectives without notable costs or penalties.
This summary encapsulates the key points from the provided episode data, highlighting the range and consistency of returns observed across the simulation episodes

![PpopointGoal](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/5088d745-cb4b-45ae-8ca8-f7dbc90d52d1)

