# CarButton0
1. During the first iteration, the average episode length was 1,000 with an average reward of 7.29. The frames per second (FPS) were 428, the total time elapsed was 6 seconds, and the total timesteps reached 3,000.

2. By the second iteration, the average episode length remained at 1,000, but the average reward increased to 8.43. The FPS was 429, the total time elapsed was 13 seconds, and the total timesteps reached 6,000. Training metrics included an approximate KL divergence of 0.00493, a clip fraction of 0.0346, a clip range of 0.2, an entropy loss of -2.84, an explained variance of -5.28, a learning rate of 0.0001, a loss of -0.011, 10 updates, a policy gradient loss of -0.00497, a standard deviation of 0.998, and a value loss of 0.0137.

3. In the third iteration, the average episode length stayed at 1,000, with an average reward of 8.24. The FPS was 447, the total time elapsed was 20 seconds, and the total timesteps reached 9,000. Training metrics showed an approximate KL divergence of 0.00607, a clip fraction of 0.0272, a clip range of 0.2, an entropy loss of -2.83, an explained variance of -0.547, a learning rate of 0.0001, a loss of -0.00134, 20 updates, a policy gradient loss of -0.00619, a standard deviation of 0.994, and a value loss of 0.0161.

4. By the fourth iteration, the average episode length was still 1,000, with an increased average reward of 8.68. The FPS was 450, the total time elapsed was 26 seconds, and the total timesteps reached 12,000. Training metrics included an approximate KL divergence of 0.00906, a clip fraction of 0.118, a clip range of 0.2, an entropy loss of -2.82, an explained variance of -0.511, a learning rate of 0.0001, a loss of -0.0244, 30 updates, a policy gradient loss of -0.0122, a standard deviation of 0.992, and a value loss of 0.00507.

5. In the fifth iteration, the average episode length remained at 1,000, with an average reward of 9.03. The FPS was 450, the total time elapsed was 33 seconds, and the total timesteps reached 15,000. Training metrics showed an approximate KL divergence of 0.00756, a clip fraction of 0.0696, a clip range of 0.2, an entropy loss of -2.82, an explained variance of 0.000145, a learning rate of 0.0001, a loss of -0.00511, 40 updates, a policy gradient loss of -0.00754, a standard deviation of 0.994, and a value loss of 0.00731.

6. During the sixth iteration, the average episode length was still 1,000, with an average reward of 9.36. The FPS was 451, the total time elapsed was 39 seconds, and the total timesteps reached 18,000. Training metrics included an approximate KL divergence of 0.00731, a clip fraction of 0.0655, a clip range of 0.2, an entropy loss of -2.82, an explained variance of 0.134, a learning rate of 0.0001, a loss of -0.00394, 50 updates, a policy gradient loss of -0.00692, a standard deviation of 0.989, and a value loss of 0.0115.

7. By the seventh iteration, the average episode length stayed at 1,000, with an increased average reward of 9.62. The FPS was 452, the total time elapsed was 46 seconds, and the total timesteps reached 21,000. Training metrics showed an approximate KL divergence of 0.00719, a clip fraction of 0.0639, a clip range of 0.2, an entropy loss of -2.81, an explained variance of 0.0666, a learning rate of 0.0001, a loss of -0.00518, 60 updates, a policy gradient loss of -0.0078, a standard deviation of 0.987, and a value loss of 0.0144.

8. In the eighth iteration, the average episode length remained at 1,000, with an average reward of 9.69. The FPS was 452, the total time elapsed was 53 seconds, and the total timesteps reached 24,000. Training metrics included an approximate KL divergence of 0.00605, a clip fraction of 0.0396, a clip range of 0.2, an entropy loss of -2.81, an explained variance of -0.32, a learning rate of 0.0001, a loss of -0.00066, 70 updates, a policy gradient loss of -0.00476, a standard deviation of 0.981, and a value loss of 0.0126.

9. During the ninth iteration, the average episode length stayed at 1,000, with an average reward of 9.8. The FPS was 452, the total time elapsed was 59 seconds, and the total timesteps reached 27,000. Training metrics showed an approximate KL divergence of 0.00724, a clip fraction of 0.0794, a clip range of 0.2, an entropy loss of -2.8, an explained variance of -0.518, a learning rate of 0.0001, a loss of -0.00319, 80 updates, a policy gradient loss of -0.00855, a standard deviation of 0.978, and a value loss of 0.0063.

10. By the tenth iteration, the average episode length was still 1,000, with an increased average reward of 9.98. The FPS was 450, the total time elapsed was 66 seconds, and the total timesteps reached 30,000. Training metrics included an approximate KL divergence of 0.00734, a clip fraction of 0.0888, a clip range of 0.2, an entropy loss of -2.79, an explained variance of 0.0477, a learning rate of 0.0001, a loss of -0.00366, 90 updates, a policy gradient loss of -0.01, a standard deviation of 0.975, and a value loss of 0.00904.

11. In the eleventh iteration, the average episode length remained at 1,000, with an average reward of 10. The FPS was 451, the total time elapsed was 73 seconds, and the total timesteps reached 33,000. Training metrics included an approximate KL divergence of 0.00668, a clip fraction of 0.0666, a clip range of 0.2, an entropy loss of -2.78, an explained variance of -0.0271, a learning rate of 0.0001, a loss of -0.00215, 100 updates, a policy gradient loss of -0.0069, a standard deviation of 0.972, and a value loss of 0.0398.

12. During the twelfth iteration, the average episode length was still 1,000, with an increased average reward of 10.2. The FPS was 453, the total time elapsed was 79 seconds, and the total timesteps reached 36,000. Training metrics included an approximate KL divergence of 0.00636, a clip fraction of 0.0486, a clip range of 0.2, an entropy loss of -2.78, an explained variance of 0.172, a learning rate of 0.0001, a loss of -0.012, 110 updates, a policy gradient loss of -0.00828, a standard deviation of 0.969, and a value loss of 0.0131.

13. In the thirteenth iteration, the average episode length remained at 1,000, with an average reward of 10.3. The FPS was 455, the total time elapsed was 85 seconds, and the total timesteps reached 39,000. Training metrics showed an approximate KL divergence of 0.00672, a clip fraction of 0.0721, a clip range of 0.2, an entropy loss of -2.77, an explained variance of 0.0464, a learning rate of 0.0001, a loss of 0.00458, 120 updates, a policy gradient loss of -0.00909, a standard deviation of 0.967, and a value loss of 0.0225.

14. By the fourteenth iteration, the average episode length was still 1,000, with an increased average reward of 10.4. The FPS was 455, the total time elapsed was 92 seconds, and the total timesteps reached 42,000. Training metrics included an approximate KL divergence of 0.00536, a clip fraction of

 0.0297, a clip range of 0.2, an entropy loss of -2.76, an explained variance of -0.365, a learning rate of 0.0001, a loss of -0.000326, 130 updates, a policy gradient loss of -0.00384, a standard deviation of 0.966, and a value loss of 0.0064.

15. In the fifteenth iteration, the average episode length remained at 1,000, with an average reward of 10.5. The FPS was 456, the total time elapsed was 98 seconds, and the total timesteps reached 45,000. Training metrics showed an approximate KL divergence of 0.00658, a clip fraction of 0.0757, a clip range of 0.2, an entropy loss of -2.76, an explained variance of 0.217, a learning rate of 0.0001, a loss of -0.00255, 140 updates, a policy gradient loss of -0.00661, a standard deviation of 0.966, and a value loss of 0.0217.

16. During the sixteenth iteration, the average episode length remained at 1,000, with an increased average reward of 10.7. The FPS was 456, the total time elapsed was 105 seconds, and the total timesteps reached 48,000. Training metrics included an approximate KL divergence of 0.00716, a clip fraction of 0.0871, a clip range of 0.2, an entropy loss of -2.76, an explained variance of -0.109, a learning rate of 0.0001, a loss of -0.006, 150 updates, a policy gradient loss of -0.00913, a standard deviation of 0.963, and a value loss of 0.00902.

17. In the seventeenth iteration, the average episode length stayed at 1,000, with an average reward of 10.7. The FPS was 456, the total time elapsed was 111 seconds, and the total timesteps reached 51,000. Training metrics showed an approximate KL divergence of 0.00574, a clip fraction of 0.0328, a clip range of 0.2, an entropy loss of -2.75, an explained variance of -0.425, a learning rate of 0.0001, a loss of -0.00409, 160 updates, a policy gradient loss of -0.00391, a standard deviation of 0.962, and a value loss of 0.00638.

18. By the eighteenth iteration, the average episode length was still 1,000, with an increased average reward of 10.8. The FPS was 456, the total time elapsed was 118 seconds, and the total timesteps reached 54,000. Training metrics showed an approximate KL divergence of 0.00662, a clip fraction of 0.0726, a clip range of 0.2, an entropy loss of -2.75, an explained variance of -0.333, a learning rate of 0.0001, a loss of -0.001, 170 updates, a policy gradient loss of -0.00755, a standard deviation of 0.959, and a value loss of 0.00543.

19. In the nineteenth iteration, the average episode length remained at 1,000, with an average reward of 10.8. The FPS was 457, the total time elapsed was 125 seconds, and the total timesteps reached 57,000. Training metrics included an approximate KL divergence of 0.00773, a clip fraction of 0.106, a clip range of 0.2, an entropy loss of -2.74, an explained variance of 0.0222, a learning rate of 0.0001, a loss of -0.0113, 180 updates, a policy gradient loss of -0.0117, a standard deviation of 0.96, and a value loss of 0.0145.

20. During the twentieth iteration, the average episode length remained at 1,000, with an increased average reward of 10.9. The FPS was 457, the total time elapsed was 131 seconds, and the total timesteps reached 60,000. Training metrics included an approximate KL divergence of 0.0066, a clip fraction of 0.0808, a clip range of 0.2, an entropy loss of -2.74, an explained variance of -0.0291, a learning rate of 0.0001, a loss of -0.00764, 190 updates, a policy gradient loss of -0.00883, a standard deviation of 0.958, and a value loss of 0.0115.

[Screencast from 06-30-2024 09:18:42 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/9fc90ac6-654a-4185-8de9-3adb16292186)

[Uploading Screencast from 06-30-2024 09:19:49 PM.webm…]()


Here's a summarized version of the training iterations:

1. **First Iteration**: Average reward 7.29, FPS 428, total time 6s, timesteps 3,000.
2. **Second Iteration**: Average reward 8.43, FPS 429, total time 13s, timesteps 6,000.
3. **Third Iteration**: Average reward 8.24, FPS 447, total time 20s, timesteps 9,000.
4. **Fourth Iteration**: Average reward 8.68, FPS 450, total time 26s, timesteps 12,000.
5. **Fifth Iteration**: Average reward 9.03, FPS 450, total time 33s, timesteps 15,000.
6. **Sixth Iteration**: Average reward 9.36, FPS 451, total time 39s, timesteps 18,000.
7. **Seventh Iteration**: Average reward 9.62, FPS 452, total time 46s, timesteps 21,000.
8. **Eighth Iteration**: Average reward 9.69, FPS 452, total time 53s, timesteps 24,000.
9. **Ninth Iteration**: Average reward 9.8, FPS 452, total time 59s, timesteps 27,000.
10. **Tenth Iteration**: Average reward 9.98, FPS 450, total time 66s, timesteps 30,000.
11. **Eleventh Iteration**: Average reward 10, FPS 451, total time 73s, timesteps 33,000.
12. **Twelfth Iteration**: Average reward 10.2, FPS 453, total time 79s, timesteps 36,000.
13. **Thirteenth Iteration**: Average reward 10.3, FPS 455, total time 85s, timesteps 39,000.
14. **Fourteenth Iteration**: Average reward 10.4, FPS 455, total time 92s, timesteps 42,000.
15. **Fifteenth Iteration**: Average reward 10.5, FPS 456, total time 98s, timesteps 45,000.
16. **Sixteenth Iteration**: Average reward 10.7, FPS 456, total time 105s, timesteps 48,000.
17. **Seventeenth Iteration**: Average reward 10.7, FPS 456, total time 111s, timesteps 51,000.
18. **Eighteenth Iteration**: Average reward 10.8, FPS 456, total time 118s, timesteps 54,000.
19. **Nineteenth Iteration**: Average reward 10.8, FPS 457, total time 125s, timesteps 57,000.
20. **Twentieth Iteration**: Average reward 10.9, FPS 457, total time 131s, timesteps 60,000.

Overall, the average reward consistently increased across iterations while FPS and timesteps also increased steadily.


The given data summarizes the returns and costs over 25 episodes, focusing on performance evaluation. Here’s a concise overview:

### Key Insights:
1. **Returns**:
   - The returns vary between episodes, ranging from a low of 9.920 (Episode 4) to a high of 19.445 (Episode 14).
   - The returns exhibit a general trend of improvement with some fluctuations.

2. **Episode Highlights**:
   - **Highest Return**: Episode 14 recorded the highest return at 19.445.
   - **Lowest Return**: Episode 4 had the lowest return at 9.920.
   - **Notable Episodes**: Episodes 12 (18.860) and 16 (18.380) also showed significantly high returns.

3. **Consistency**:
   - The costs remained consistent at 0.000 across all episodes, indicating no expenditure was incurred.

4. **Performance Distribution**:
   - Several episodes (e.g., 2, 3, 8, 9, 12, 14, 16, 18) had returns above 15, indicating strong performance.
   - Some episodes (e.g., 4, 5, 13, 19, 20) had returns below 13, highlighting areas for potential improvement.

### Summary:
Overall, the performance across the episodes shows an upward trend with notable peaks and some lower-performing episodes. The average return reflects this variability, showcasing periods of high efficiency and areas that may benefit from further analysis and optimization.

![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/88198d51-f8e1-4f9e-abc9-c056ec8cebe3)

