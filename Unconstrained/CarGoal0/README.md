1. During the first iteration, the mean episode length was 1000, the mean episode reward was 7.21, and the total time elapsed was 6 seconds with 3000 timesteps completed at a speed of 484 frames per second (fps).

2. After the second iteration, the mean episode length remained at 1000, the mean episode reward increased to 7.94, and the time elapsed was 12 seconds with 6000 total timesteps and a speed of 487 fps. Training metrics included an approximate KL divergence of 0.0061121, a clip fraction of 0.0645, an entropy loss of -2.84, and an explained variance of -6.12. The learning rate was 0.0001, and there were 10 updates with a policy gradient loss of -0.00639, a standard deviation of 0.999, and a value loss of 0.00739.

3. By the third iteration, the mean episode reward had increased to 8.45, with the total time elapsed at 17 seconds, 9000 timesteps, and 503 fps. Training metrics showed an approximate KL divergence of 0.007967507, a clip fraction of 0.0917, an entropy loss of -2.83, and an explained variance of -0.582. The learning rate remained 0.0001, with 20 updates, a policy gradient loss of -0.00998, a standard deviation of 0.996, and a value loss of 0.00422.

4. At the fourth iteration, the mean episode reward increased to 8.78, with 23 seconds elapsed, 12000 timesteps, and 513 fps. Training metrics included an approximate KL divergence of 0.0093781, a clip fraction of 0.117, an entropy loss of -2.83, and an explained variance of -0.392. The learning rate was 0.0001, with 30 updates, a policy gradient loss of -0.0116, a standard deviation of 0.993, and a value loss of 0.0038.

5. In the fifth iteration, the mean episode reward reached 9, with 29 seconds elapsed, 15000 timesteps, and 516 fps. Training metrics showed an approximate KL divergence of 0.0049604964, a clip fraction of 0.047, an entropy loss of -2.82, and an explained variance of -0.817. The learning rate was 0.0001, with 40 updates, a policy gradient loss of -0.00706, a standard deviation of 0.99, and a value loss of 0.00429.

6. By the sixth iteration, the mean episode reward increased to 9.19, with 34 seconds elapsed, 18000 timesteps, and 519 fps. Training metrics included an approximate KL divergence of 0.0065760524, a clip fraction of 0.0531, an entropy loss of -2.81, and an explained variance of -0.432. The learning rate was 0.0001, with 50 updates, a policy gradient loss of -0.00643, a standard deviation of 0.987, and a value loss of 0.00402.

7. In the seventh iteration, the mean episode reward reached 9.3, with 40 seconds elapsed, 21000 timesteps, and 523 fps. Training metrics showed an approximate KL divergence of 0.00574683, a clip fraction of 0.0607, an entropy loss of -2.81, and an explained variance of -0.192. The learning rate remained at 0.0001, with 60 updates, a policy gradient loss of -0.00646, a standard deviation of 0.984, and a value loss of 0.00676.

8. By the eighth iteration, the mean episode reward had risen to 9.41, with 45 seconds elapsed, 24000 timesteps, and 526 fps. Training metrics included an approximate KL divergence of 0.0056104567, a clip fraction of 0.0604, an entropy loss of -2.81, and an explained variance of -0.327. The learning rate was 0.0001, with 70 updates, a policy gradient loss of -0.00713, a standard deviation of 0.984, and a value loss of 0.00385.

9. During the ninth iteration, the mean episode reward was 9.48, with 51 seconds elapsed, 27000 timesteps, and 526 fps. Training metrics showed an approximate KL divergence of 0.0052507715, a clip fraction of 0.0592, an entropy loss of -2.81, and an explained variance of -0.179. The learning rate was 0.0001, with 80 updates, a policy gradient loss of -0.00856, a standard deviation of 0.985, and a value loss of 0.00541.

10. At the tenth iteration, the mean episode reward increased to 9.54, with 56 seconds elapsed, 30000 timesteps, and 527 fps. Training metrics included an approximate KL divergence of 0.0076043545, a clip fraction of 0.0932, an entropy loss of -2.81, and an explained variance of -0.204. The learning rate was 0.0001, with 90 updates, a policy gradient loss of -0.00915, a standard deviation of 0.987, and a value loss of 0.00578.

11. By the eleventh iteration, the mean episode reward reached 9.55, with 62 seconds elapsed, 33000 timesteps, and 528 fps. Training metrics showed an approximate KL divergence of 0.0052525615, a clip fraction of 0.0441, an entropy loss of -2.81, and an explained variance of -0.158. The learning rate was 0.0001, with 100 updates, a policy gradient loss of -0.00637, a standard deviation of 0.987, and a value loss of 0.0056.

12. In the twelfth iteration, the mean episode reward rose to 9.62, with 67 seconds elapsed, 36000 timesteps, and 530 fps. Training metrics included an approximate KL divergence of 0.007847969, a clip fraction of 0.0751, an entropy loss of -2.81, and an explained variance of 0.154. The learning rate remained at 0.0001, with 110 updates, a policy gradient loss of -0.00807, a standard deviation of 0.987, and a value loss of 0.00695.

13. During the thirteenth iteration, the mean episode reward increased to 9.71, with 73 seconds elapsed, 39000 timesteps, and 530 fps. Training metrics showed an approximate KL divergence of 0.006839562, a clip fraction of 0.0743, an entropy loss of -2.81, and an explained variance of -0.0353. The learning rate was 0.0001, with 120 updates, a policy gradient loss of -0.00584, a standard deviation of 0.985, and a value loss of 0.00841.

14. At the fourteenth iteration, the mean episode reward reached 9.79, with 79 seconds elapsed, 42000 timesteps, and 531 fps. Training metrics included an approximate KL divergence of 0.0056538177, a clip fraction of 0.081, an entropy loss of -2.81, and an explained variance of -0.201. The learning rate was 0.0001, with 130 updates, a policy gradient loss of -0.00849, a standard deviation of 0.983, and a value loss of 0.00924.

15. By the fifteenth iteration, the mean episode reward was 9.83, with 84 seconds elapsed, 45000 timesteps, and 531 fps. Training metrics showed an approximate KL divergence of 0.006825491, a clip fraction of 0.0723, an entropy loss of -2.8, and an explained variance of -0.106. The learning rate was 0.0001, with 140 updates, a policy gradient loss of -0.00745, a standard deviation of 0.98, and a value loss of 0.00887.

16. In the sixteenth iteration, the mean episode reward increased to 9.97, with 90 seconds elapsed, 48000 timesteps, and 531 fps. Training metrics included an approximate KL divergence of 0.005811161, a clip fraction of 0.0604, an entropy loss of -2.79, and an explained variance of 0.0287. The learning rate was 0.0001, with 150 updates, a policy gradient loss of -0.00776, a standard deviation of 0.978, and a value loss of 0.0101.

17. During the seventeenth iteration, the mean episode reward rose to 10.1, with 95 seconds elapsed, 51000 timesteps, and 532 fps. Training metrics showed an approximate KL divergence of 0.0067360452, a clip fraction of 0.0327, an entropy loss of -2.79, and an explained variance of 0.0487. The learning rate was

 0.0001, with 160 updates, a policy gradient loss of -0.00594, a standard deviation of 0.978, and a value loss of 0.0126.

18. By the eighteenth iteration, the mean episode reward had increased to 10.2, with 101 seconds elapsed, 54000 timesteps, and 532 fps. Training metrics included an approximate KL divergence of 0.0077170513, a clip fraction of 0.0613, an entropy loss of -2.78, and an explained variance of -0.107. The learning rate was 0.0001, with 170 updates, a policy gradient loss of -0.00585, a standard deviation of 0.978, and a value loss of 0.0123.

19. At the nineteenth iteration, the mean episode reward reached 10.3, with 106 seconds elapsed, 57000 timesteps, and 532 fps. Training metrics showed an approximate KL divergence of 0.004870126, a clip fraction of 0.0302, an entropy loss of -2.78, and an explained variance of 0.0451. The learning rate was 0.0001, with 180 updates, a policy gradient loss of -0.00533, a standard deviation of 0.98, and a value loss of 0.0122.

20. In the twentieth iteration, the mean episode reward was 10.3, with 111 seconds elapsed, 60000 timesteps, and 532 fps. Training metrics included an approximate KL divergence of 0.005194244, a clip fraction of 0.0493, an entropy loss of -2.78, and an explained variance of 0.0345. The learning rate was 0.0001, with 190 updates, a policy gradient loss of -0.00623, a standard deviation of 0.983, and a value loss of 0.0127.


https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/a82effa0-6bce-474f-8b02-24d70bc4f0f6


  The training involved 20 iterations where the mean episode reward steadily increased from 7.21 to 10.30. Each iteration involved increasing amounts of time and timesteps, with a stable FPS around 530. This indicates consistent improvement in the agent's performance and efficient training throughput over the course of the training process.


  The returns for each episode of the model or agent were as follows:

The returns ranged from a minimum of 12.394 in Episode 18 to a maximum of 19.487 in Episode 10.
The average return across all episodes was approximately 15.912.
Episodes with returns above the average include Episodes 2, 3, 7, 8, 10, 16, 17, 21, and 22.
Episodes with returns below the average include Episodes 5, 9, 11, 12, 13, 14, 18, 19, 20, 23, 24, and 25.
Overall, the returns varied across episodes, indicating fluctuations in the performance of the model or agent during different training or evaluation phases.

![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/ac3bc5bf-4b13-4de4-8918-044743589b39)

