# PointGoal0

Here is the provided information converted into sentences:

After 1 iteration, the episode length mean was 1000, and the episode reward mean was 9.17. The time elapsed was 5 seconds, with 3000 total timesteps and a frame rate of 549 fps.

After 2 iterations, the episode length mean was 1000, and the episode reward mean was 8.97. The time elapsed was 11 seconds, with 6000 total timesteps and a frame rate of 545 fps. Training metrics showed an approximate KL divergence of 0.0051705865, a clip fraction of 0.0512, an entropy loss of -2.84, an explained variance of -10.8, a learning rate of 0.0001, a loss of 0.00428, and a policy gradient loss of -0.00463.

After 3 iterations, the episode length mean was 1000, and the episode reward mean was 9.33. The time elapsed was 15 seconds, with 9000 total timesteps and a frame rate of 565 fps. Training metrics showed an approximate KL divergence of 0.0055264146, a clip fraction of 0.0338, an entropy loss of -2.85, an explained variance of 0.0103, a learning rate of 0.0001, a loss of -0.00299, and a policy gradient loss of -0.0034.

After 4 iterations, the episode length mean was 1000, and the episode reward mean was 9.27. The time elapsed was 21 seconds, with 12000 total timesteps and a frame rate of 565 fps. Training metrics showed an approximate KL divergence of 0.0041381484, a clip fraction of 0.0354, an entropy loss of -2.85, an explained variance of 0.143, a learning rate of 0.0001, a loss of -0.0033, and a policy gradient loss of -0.00379.

After 5 iterations, the episode length mean was 1000, and the episode reward mean was 9.35. The time elapsed was 26 seconds, with 15000 total timesteps and a frame rate of 565 fps. Training metrics showed an approximate KL divergence of 0.0054386063, a clip fraction of 0.0535, an entropy loss of -2.85, an explained variance of 0.578, a learning rate of 0.0001, a loss of -0.00816, and a policy gradient loss of -0.00546.

After 6 iterations, the episode length mean was 1000, and the episode reward mean was 9.53. The time elapsed was 31 seconds, with 18000 total timesteps and a frame rate of 564 fps. Training metrics showed an approximate KL divergence of 0.0054409867, a clip fraction of 0.0334, an entropy loss of -2.84, an explained variance of 0.488, a learning rate of 0.0001, a loss of -0.00865, and a policy gradient loss of -0.00347.

After 7 iterations, the episode length mean was 1000, and the episode reward mean was 9.49. The time elapsed was 36 seconds, with 21000 total timesteps and a frame rate of 569 fps. Training metrics showed an approximate KL divergence of 0.0051120883, a clip fraction of 0.0349, an entropy loss of -2.84, an explained variance of 0.0427, a learning rate of 0.0001, a loss of -0.00436, and a policy gradient loss of -0.0037.

After 8 iterations, the episode length mean was 1000, and the episode reward mean was 9.55. The time elapsed was 41 seconds, with 24000 total timesteps and a frame rate of 575 fps. Training metrics showed an approximate KL divergence of 0.007952513, a clip fraction of 0.118, an entropy loss of -2.83, an explained variance of 0.0869, a learning rate of 0.0001, a loss of -0.0178, and a policy gradient loss of -0.00947.

After 9 iterations, the episode length mean was 1000, and the episode reward mean was 9.63. The time elapsed was 46 seconds, with 27000 total timesteps and a frame rate of 579 fps. Training metrics showed an approximate KL divergence of 0.0065876646, a clip fraction of 0.0741, an entropy loss of -2.83, an explained variance of 0.0756, a learning rate of 0.0001, a loss of -0.00221, and a policy gradient loss of -0.0053.

After 10 iterations, the episode length mean was 1000, and the episode reward mean was 9.67. The time elapsed was 51 seconds, with 30000 total timesteps and a frame rate of 582 fps. Training metrics showed an approximate KL divergence of 0.004502637, a clip fraction of 0.0381, an entropy loss of -2.83, an explained variance of 0.116, a learning rate of 0.0001, a loss of -0.00335, and a policy gradient loss of -0.003.

After 11 iterations, the episode length mean was 1000, and the episode reward mean was 9.83. The time elapsed was 56 seconds, with 33000 total timesteps and a frame rate of 586 fps. Training metrics showed an approximate KL divergence of 0.004136361, a clip fraction of 0.0388, an entropy loss of -2.82, an explained variance of 0.00845, a learning rate of 0.0001, a loss of -0.00793, and a policy gradient loss of -0.00414.

After 12 iterations, the episode length mean was 1000, and the episode reward mean was 9.99. The time elapsed was 60 seconds, with 36000 total timesteps and a frame rate of 590 fps. Training metrics showed an approximate KL divergence of 0.007121361, a clip fraction of 0.0746, an entropy loss of -2.81, an explained variance of 0.0687, a learning rate of 0.0001, a loss of 0.00126, and a policy gradient loss of -0.00665.

After 13 iterations, the episode length mean was 1000, and the episode reward mean was 10.1. The time elapsed was 65 seconds, with 39000 total timesteps and a frame rate of 592 fps. Training metrics showed an approximate KL divergence of 0.005681712, a clip fraction of 0.0321, an entropy loss of -2.81, an explained variance of 0.016, a learning rate of 0.0001, a loss of -0.0064, and a policy gradient loss of -0.0049.

After 14 iterations, the episode length mean was 1000, and the episode reward mean was 10.4. The time elapsed was 70 seconds, with 42000 total timesteps and a frame rate of 595 fps. Training metrics showed an approximate KL divergence of 0.009117456, a clip fraction of 0.112, an entropy loss of -2.81, an explained variance of -0.0353, a learning rate of 0.0001, a loss of 0.00165, and a policy gradient loss of -0.00643.

After 15 iterations, the episode length mean was 1000, and the episode reward mean was 10.5. The time elapsed was 75 seconds, with 45000 total timesteps and a frame rate of 596 fps. Training metrics showed an approximate KL divergence of 0.0033389921, a clip fraction of 0.0144, an entropy loss of -2.81, an explained variance of 0.121, a learning rate of 0.0001, a loss of 0.0066, and a policy gradient loss of -0.00255.

After 16 iterations, the episode length mean was 1000, and the episode reward mean was 10.8. The time elapsed was 80 seconds, with 48000 total timesteps and a frame rate of 597 fps. Training metrics showed an approximate KL divergence of 0.005940133, a clip fraction of 0.044, an entropy loss of -2.81, an explained variance of 0.115, a learning rate of 0.0001, a loss of -0.00439, and a policy gradient loss of -0.00387.

After 17 iterations, the episode length mean was 1000, and the episode reward mean was 11. The time elapsed was 85 seconds, with 51000 total timesteps and a frame rate of 599 fps. Training metrics showed an approximate KL divergence of 0.0067209513, a clip fraction of 0.0716, an entropy loss of -2.8, an explained variance of 0.213, a learning rate of 0.0001, a loss of -0.00925, and a policy gradient loss of -0.00628.

After 18 iterations, the episode length mean was 

1000, and the episode reward mean was 11.1. The time elapsed was 90 seconds, with 54000 total timesteps and a frame rate of 600 fps. Training metrics showed an approximate KL divergence of 0.00802812, a clip fraction of 0.0856, an entropy loss of -2.79, an explained variance of 0.0672, a learning rate of 0.0001, a loss of -0.012, and a policy gradient loss of -0.00826.

After 19 iterations, the episode length mean was 1000, and the episode reward mean was 11.2. The time elapsed was 95 seconds, with 57000 total timesteps and a frame rate of 601 fps. Training metrics showed an approximate KL divergence of 0.006490335, a clip fraction of 0.0518, an entropy loss of -2.79, an explained variance of 0.0143, a learning rate of 0.0001, a loss of -0.0071, and a policy gradient loss of -0.00587.

After 20 iterations, the episode length mean was 1000, and the episode reward mean was 11.3. The time elapsed was 99 seconds, with 60000 total timesteps and a frame rate of 601 fps. Training metrics showed an approximate KL divergence of 0.0074380283, a clip fraction of 0.0906, an entropy loss of -2.78, an explained variance of 0.0857, a learning rate of 0.0001, a loss of -0.00543, and a policy gradient loss of -0.00657.

[Screencast from 07-02-2024 06:53:31 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/169de921-5691-4810-9e61-3868aa83267a)

## Summary

- **Iterations 1-5**: 
  - Episode length mean: 1000
  - Episode reward mean: Ranged from 9.17 to 9.35
  - Time elapsed: Increased from 5 to 26 seconds
  - Total timesteps: Increased from 3000 to 15000
  - Frame rate: Approximately 545-565 fps

- **Iterations 6-10**:
  - Episode length mean: 1000
  - Episode reward mean: Ranged from 9.49 to 9.67
  - Time elapsed: Increased from 31 to 51 seconds
  - Total timesteps: Increased from 18000 to 30000
  - Frame rate: Approximately 564-582 fps

- **Iterations 11-15**:
  - Episode length mean: 1000
  - Episode reward mean: Ranged from 9.83 to 10.5
  - Time elapsed: Increased from 56 to 75 seconds
  - Total timesteps: Increased from 33000 to 45000
  - Frame rate: Approximately 586-596 fps

- **Iterations 16-20**:
  - Episode length mean: 1000
  - Episode reward mean: Ranged from 10.8 to 11.3
  - Time elapsed: Increased from 80 to 99 seconds
  - Total timesteps: Increased from 48000 to 60000
  - Frame rate: Approximately 597-601 fps

Training metrics showed typical values for KL divergence, clip fraction, entropy loss, explained variance, learning rate, loss, and policy gradient loss, with some variations across iterations.

The episode returns show the performance of the model over 25 episodes.

- **Episodes 1-5**: The return started at 21.407 and gradually increased to around 24.226.
- **Episodes 6-10**: The return fluctuated, peaking at 26.708 in Episode 7 and settling around 24.452 by Episode 10.
- **Episodes 11-15**: Returns varied, with a notable drop to 19.712 in Episode 15.
- **Episodes 16-20**: The returns increased again, reaching a high of 27.256 in Episode 20.
- **Episodes 21-25**: Performance remained high, with returns peaking at 28.056 in Episode 24 and slightly decreasing to 25.374 by Episode 25.

Overall, the model showed an upward trend with some fluctuations, achieving higher returns towards the later episodes.

![Result](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/5b03b8ac-456b-420d-891a-3862a46b8853)

