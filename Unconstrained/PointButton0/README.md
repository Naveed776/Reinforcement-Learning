1. During the first iteration, the mean episode length was 1000 with a mean episode reward of 10.1. The frame rate was 519 FPS, the iteration count was 1, the time elapsed was 5 seconds, and the total timesteps were 3000.

2. In the second iteration, the mean episode length remained 1000, but the mean episode reward decreased slightly to 9.83. The FPS improved to 521, with 2 iterations completed, 11 seconds elapsed, and 6000 total timesteps. The training metrics included an approximate KL divergence of 0.0052, clip fraction of 0.0485, clip range of 0.2, entropy loss of -2.84, explained variance of -14.5, learning rate of 0.0001, loss of 0.00518, 10 updates, policy gradient loss of -0.00428, standard deviation of 1, and value loss of 0.00928.

3. By the third iteration, the mean episode length was still 1000, with a mean episode reward of 9.73. The FPS increased to 541, with 3 iterations completed, 16 seconds elapsed, and 9000 total timesteps. Training metrics were an approximate KL divergence of 0.00448, clip fraction of 0.0352, clip range of 0.2, entropy loss of -2.84, explained variance of 0.00741, learning rate of 0.0001, loss of -0.00635, 20 updates, policy gradient loss of -0.00334, standard deviation of 1, and value loss of 0.00489.

4. In the fourth iteration, the mean episode length stayed at 1000 with a mean episode reward of 9.95. FPS increased to 552, with 4 iterations completed, 21 seconds elapsed, and 12000 total timesteps. Training metrics included an approximate KL divergence of 0.00399, clip fraction of 0.0291, clip range of 0.2, entropy loss of -2.84, explained variance of 0.246, learning rate of 0.0001, loss of 0.00173, 30 updates, policy gradient loss of -0.00327, standard deviation of 1, and value loss of 0.00511.

5. During the fifth iteration, the mean episode length remained at 1000, with a mean episode reward of 9.96. The FPS rose to 556, with 5 iterations completed, 26 seconds elapsed, and 15000 total timesteps. Training metrics showed an approximate KL divergence of 0.0043, clip fraction of 0.0368, clip range of 0.2, entropy loss of -2.84, explained variance of 0.261, learning rate of 0.0001, loss of -0.000292, 40 updates, policy gradient loss of -0.00382, standard deviation of 1, and value loss of 0.00772.

6. By the sixth iteration, the mean episode length stayed at 1000 with a mean episode reward of 9.97. FPS increased to 558, with 6 iterations completed, 32 seconds elapsed, and 18000 total timesteps. Training metrics included an approximate KL divergence of 0.0044, clip fraction of 0.0204, clip range of 0.2, entropy loss of -2.84, explained variance of 0.0426, learning rate of 0.0001, loss of 0.00275, 50 updates, policy gradient loss of -0.0035, standard deviation of 0.999, and value loss of 0.00836.

7. In the seventh iteration, the mean episode length remained at 1000, with a mean episode reward returning to 10.1. The FPS rose to 562, with 7 iterations completed, 37 seconds elapsed, and 21000 total timesteps. Training metrics showed an approximate KL divergence of 0.007, clip fraction of 0.0664, clip range of 0.2, entropy loss of -2.84, explained variance of 0.173, learning rate of 0.0001, loss of -0.00718, 60 updates, policy gradient loss of -0.0049, standard deviation of 1, and value loss of 0.0051.

8. In the eighth iteration, the mean episode length stayed at 1000 with a mean episode reward increasing to 10.5. FPS rose to 564, with 8 iterations completed, 42 seconds elapsed, and 24000 total timesteps. Training metrics included an approximate KL divergence of 0.0058, clip fraction of 0.0624, clip range of 0.2, entropy loss of -2.83, explained variance of 0.114, learning rate of 0.0001, loss of -0.00211, 70 updates, policy gradient loss of -0.0061, standard deviation of 0.995, and value loss of 0.0115.

9. By the ninth iteration, the mean episode length remained at 1000 with a mean episode reward increasing to 10.6. FPS rose to 565, with 9 iterations completed, 47 seconds elapsed, and 27000 total timesteps. Training metrics showed an approximate KL divergence of 0.0068, clip fraction of 0.078, clip range of 0.2, entropy loss of -2.82, explained variance of 0.0742, learning rate of 0.0001, loss of 0.0154, 80 updates, policy gradient loss of -0.00642, standard deviation of 0.991, and value loss of 0.0301.

10. In the tenth iteration, the mean episode length stayed at 1000 with a mean episode reward increasing to 10.7. FPS increased to 566, with 10 iterations completed, 52 seconds elapsed, and 30000 total timesteps. Training metrics included an approximate KL divergence of 0.0066, clip fraction of 0.0436, clip range of 0.2, entropy loss of -2.82, explained variance of -0.16, learning rate of 0.0001, loss of 0.000407, 90 updates, policy gradient loss of -0.00283, standard deviation of 0.989, and value loss of 0.0137.

11. By the eleventh iteration, the mean episode length remained at 1000 with a mean episode reward increasing to 11. FPS increased to 568, with 11 iterations completed, 58 seconds elapsed, and 33000 total timesteps. Training metrics showed an approximate KL divergence of 0.004, clip fraction of 0.0356, clip range of 0.2, entropy loss of -2.81, explained variance of 0.163, learning rate of 0.0001, loss of 0.00326, 100 updates, policy gradient loss of -0.00292, standard deviation of 0.988, and value loss of 0.0157.

12. In the twelfth iteration, the mean episode length stayed at 1000 with a mean episode reward increasing to 11.2. FPS rose to 570, with 12 iterations completed, 63 seconds elapsed, and 36000 total timesteps. Training metrics included an approximate KL divergence of 0.0041, clip fraction of 0.0301, clip range of 0.2, entropy loss of -2.81, explained variance of 0.282, learning rate of 0.0001, loss of 0.00204, 110 updates, policy gradient loss of -0.00421, standard deviation of 0.983, and value loss of 0.034.

13. By the thirteenth iteration, the mean episode length remained at 1000 with a mean episode reward increasing to 11.4. FPS increased to 570, with 13 iterations completed, 68 seconds elapsed, and 39000 total timesteps. Training metrics showed an approximate KL divergence of 0.0059, clip fraction of 0.0422, clip range of 0.2, entropy loss of -2.8, explained variance of 0.381, learning rate of 0.0001, loss of 0.00405, 120 updates, policy gradient loss of -0.00397, standard deviation of 0.984, and value loss of 0.0213.

14. During the fourteenth iteration, the mean episode length remained at 1000 with a mean episode reward holding at 11.4. The FPS increased to 571, with 14 iterations completed, 73 seconds elapsed, and 42000 total timesteps. Training metrics included an approximate KL divergence of 0.0049, clip fraction of 0.0554, clip range of 0.2, entropy loss of -2.8, explained variance of 0.385, learning rate of 0.0001, loss of 0.00324, 130 updates, policy gradient loss of -0.00543, standard deviation of 0.983, and value loss of 0.0309.

15. In the fifteenth iteration, the mean episode length stayed at 1000 with a mean episode reward increasing to 11.6. FPS increased to 571, with 15 iterations completed, 78 seconds elapsed, and 45000 total timesteps

. Training metrics showed an approximate KL divergence of 0.0049, clip fraction of 0.0453, clip range of 0.2, entropy loss of -2.79, explained variance of 0.434, learning rate of 0.0001, loss of -0.000259, 140 updates, policy gradient loss of -0.00501, standard deviation of 0.983, and value loss of 0.0198.

16. By the sixteenth iteration, the mean episode length remained at 1000 with a mean episode reward staying at 11.6. FPS increased to 571, with 16 iterations completed, 83 seconds elapsed, and 48000 total timesteps. Training metrics included an approximate KL divergence of 0.0048, clip fraction of 0.0562, clip range of 0.2, entropy loss of -2.78, explained variance of 0.42, learning rate of 0.0001, loss of -0.000327, 150 updates, policy gradient loss of -0.00534, standard deviation of 0.982, and value loss of 0.0154.

17. During the seventeenth iteration, the mean episode length remained at 1000 with a mean episode reward decreasing slightly to 11.5. The FPS increased to 572, with 17 iterations completed, 88 seconds elapsed, and 51000 total timesteps. Training metrics showed an approximate KL divergence of 0.0038, clip fraction of 0.0456, clip range of 0.2, entropy loss of -2.78, explained variance of 0.528, learning rate of 0.0001, loss of -0.000112, 160 updates, policy gradient loss of -0.0039, standard deviation of 0.981, and value loss of 0.0162.

18. By the eighteenth iteration, the mean episode length remained at 1000 with a mean episode reward holding at 11.5. FPS increased to 572, with 18 iterations completed, 93 seconds elapsed, and 54000 total timesteps. Training metrics included an approximate KL divergence of 0.0031, clip fraction of 0.0362, clip range of 0.2, entropy loss of -2.77, explained variance of 0.587, learning rate of 0.0001, loss of 0.00342, 170 updates, policy gradient loss of -0.00352, standard deviation of 0.98, and value loss of 0.0147.

19. In the nineteenth iteration, the mean episode length remained at 1000 with a mean episode reward staying at 11.5. The FPS increased to 573, with 19 iterations completed, 98 seconds elapsed, and 57000 total timesteps. Training metrics showed an approximate KL divergence of 0.0042, clip fraction of 0.0376, clip range of 0.2, entropy loss of -2.77, explained variance of 0.615, learning rate of 0.0001, loss of 0.00018, 180 updates, policy gradient loss of -0.0034, standard deviation of 0.978, and value loss of 0.0137.

20. During the twentieth iteration, the mean episode length remained at 1000 with a mean episode reward holding at 11.5. The FPS increased to 573, with 20 iterations completed, 103 seconds elapsed, and 60000 total timesteps. Training metrics included an approximate KL divergence of 0.0042, clip fraction of 0.0464, clip range of 0.2, entropy loss of -2.76, explained variance of 0.615, learning rate of 0.0001, loss of -0.00424, 190 updates, policy gradient loss of -0.00413, standard deviation of 0.977, and value loss of 0.0103.


[Screencast from 07-02-2024 07:04:15 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/c7d63ca7-def3-4b82-9ee8-95295245bf06)



1. **Iteration 1-3:** Mean episode reward starts at 10.1, decreases slightly to 9.73. FPS increases from 519 to 541. Training metrics show initial KL divergence around 0.005, clip fraction ~0.04, entropy loss -2.84, explained variance improving slightly, learning rate 0.0001.

2. **Iteration 4-7:** Mean episode reward fluctuates between 9.95 and 10.1. FPS increases from 552 to 562. KL divergence remains around 0.004-0.007, clip fraction ~0.04, entropy loss -2.84, explained variance fluctuates, policy gradient loss decreases.

3. **Iteration 8-11:** Mean episode reward increases from 10.5 to 11. FPS rises to 568. KL divergence around 0.004-0.007, clip fraction ~0.04-0.07, entropy loss -2.81 to -2.83, explained variance improves, learning rate stable at 0.0001.

4. **Iteration 12-16:** Mean episode reward continues to increase, reaching 11.6. FPS increases to 571. KL divergence stable around 0.004-0.005, clip fraction ~0.04-0.06, entropy loss -2.78 to -2.80, explained variance continues to improve, policy gradient loss remains low.

5. **Iteration 17-20:** Mean episode reward stabilizes at 11.5. FPS increases to 573. KL divergence around 0.003-0.004, clip fraction ~0.03-0.05, entropy loss -2.76 to -2.78, explained variance remains high, learning rate steady at 0.0001.

Overall, the training shows a steady improvement in mean episode rewards and FPS, with consistent training metrics indicating stable learning progress.


![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/095c3d76-ed0a-43ce-ac5b-cdd05a2543ed)



The episode returns showed considerable fluctuation. In the first few episodes, the returns started at 16.718 and 15.483. A significant spike was observed in Episode 3 with a return of 28.677, and it remained high through Episodes 4 to 8, with returns ranging from 23.683 to 28.956. Episode 9 saw a drop to 21.507, followed by another dip to 16.641 in Episode 10. However, returns rebounded strongly in Episodes 11 to 14, with values between 24.743 and 30.814. After a slight decline in Episodes 15 and 16, with returns around 20.057 to 20.082, the returns rose again in Episodes 17 to 18, reaching up to 27.724 and 26.783. Episode 19 had a notable drop to 15.647, but the returns quickly recovered in the subsequent episodes, averaging around 22.238 to 27.883, with the final Episode 25 recording a return of 25.517.
