After 1 iteration and 6 seconds elapsed, the mean episode length is 1000, the mean episode reward is 7.21, the FPS is 484, and the total timesteps are 3000.
After 2 iterations and 12 seconds elapsed, the mean episode length is 1000, the mean episode reward is 7.94, the FPS is 487, and the total timesteps are 6000. The training metrics are: approximate KL divergence of 0.0061121, clip fraction of 0.0645, clip range of 0.2, entropy loss of -2.84, explained variance of -6.12, learning rate of 0.0001, loss of -0.00873, 10 updates, policy gradient loss of -0.00639, standard deviation of 0.999, and value loss of 0.00739.
After 3 iterations and 17 seconds elapsed, the mean episode length is 1000, the mean episode reward is 8.45, the FPS is 503, and the total timesteps are 9000. The training metrics are: approximate KL divergence of 0.007967507, clip fraction of 0.0917, clip range of 0.2, entropy loss of -2.83, explained variance of -0.582, learning rate of 0.0001, loss of -0.0032, 20 updates, policy gradient loss of -0.00998, standard deviation of 0.996, and value loss of 0.00422.
After 4 iterations and 23 seconds elapsed, the mean episode length is 1000, the mean episode reward is 8.78, the FPS is 513, and the total timesteps are 12000. The training metrics are: approximate KL divergence of 0.0093781, clip fraction of 0.117, clip range of 0.2, entropy loss of -2.83, explained variance of -0.392, learning rate of 0.0001, loss of -0.0166, 30 updates, policy gradient loss of -0.0116, standard deviation of 0.993, and value loss of 0.0038.
After 5 iterations and 29 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9, the FPS is 516, and the total timesteps are 15000. The training metrics are: approximate KL divergence of 0.0049604964, clip fraction of 0.047, clip range of 0.2, entropy loss of -2.82, explained variance of -0.817, learning rate of 0.0001, loss of -0.0186, 40 updates, policy gradient loss of -0.00706, standard deviation of 0.99, and value loss of 0.00429.
After 6 iterations and 34 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.19, the FPS is 519, and the total timesteps are 18000. The training metrics are: approximate KL divergence of 0.0065760524, clip fraction of 0.0531, clip range of 0.2, entropy loss of -2.81, explained variance of -0.432, learning rate of 0.0001, loss of -0.00858, 50 updates, policy gradient loss of -0.00643, standard deviation of 0.987, and value loss of 0.00402.
After 7 iterations and 40 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.3, the FPS is 523, and the total timesteps are 21000. The training metrics are: approximate KL divergence of 0.00574683, clip fraction of 0.0607, clip range of 0.2, entropy loss of -2.81, explained variance of -0.192, learning rate of 0.0001, loss of -0.00656, 60 updates, policy gradient loss of -0.00646, standard deviation of 0.984, and value loss of 0.00676.
After 8 iterations and 45 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.41, the FPS is 526, and the total timesteps are 24000. The training metrics are: approximate KL divergence of 0.0056104567, clip fraction of 0.0604, clip range of 0.2, entropy loss of -2.81, explained variance of -0.327, learning rate of 0.0001, loss of -0.00505, 70 updates, policy gradient loss of -0.00713, standard deviation of 0.984, and value loss of 0.00385.
After 9 iterations and 51 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.48, the FPS is 526, and the total timesteps are 27000. The training metrics are: approximate KL divergence of 0.0052507715, clip fraction of 0.0592, clip range of 0.2, entropy loss of -2.81, explained variance of -0.179, learning rate of 0.0001, loss of -0.0121, 80 updates, policy gradient loss of -0.00856, standard deviation of 0.985, and value loss of 0.00541.
After 10 iterations and 56 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.54, the FPS is 527, and the total timesteps are 30000. The training metrics are: approximate KL divergence of 0.0076043545, clip fraction of 0.0932, clip range of 0.2, entropy loss of -2.81, explained variance of -0.204, learning rate of 0.0001, loss of -0.00204, 90 updates, policy gradient loss of -0.00915, standard deviation of 0.987, and value loss of 0.00578.
After 11 iterations and 62 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.55, the FPS is 528, and the total timesteps are 33000. The training metrics are: approximate KL divergence of 0.0052525615, clip fraction of 0.0441, clip range of 0.2, entropy loss of -2.81, explained variance of -0.158, learning rate of 0.0001, loss of -0.00856, 100 updates, policy gradient loss of -0.00637, standard deviation of 0.987, and value loss of 0.0056.
After 12 iterations and 67 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.62, the FPS is 530, and the total timesteps are 36000. The training metrics are: approximate KL divergence of 0.007847969, clip fraction of 0.0751, clip range of 0.2, entropy loss of -2.81, explained variance of 0.154, learning rate of 0.0001, loss of -0.00573, 110 updates, policy gradient loss of -0.00807, standard deviation of 0.987, and value loss of 0.00695.
After 13 iterations and 73 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.71, the FPS is 530, and the total timesteps are 39000. The training metrics are: approximate KL divergence of 0.006839562, clip fraction of 0.0743, clip range of 0.2, entropy loss of -2.81, explained variance of -0.0353, learning rate of 0.0001, loss of -0.00481, 120 updates, policy gradient loss of -0.00584, standard deviation of 0.985, and value loss of 0.00841.
After 14 iterations and 79 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.79, the FPS is 531, and the total timesteps are 42000. The training metrics are: approximate KL divergence of 0.0056538177, clip fraction of 0.081, clip range of 0.2, entropy loss of -2.81, explained variance of -0.201, learning rate of 0.0001, loss of -0.0189, 130 updates, policy gradient loss of -0.00849, standard deviation of 0.983, and value loss of 0.00924.
After 15 iterations and 84 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.83, the FPS is 531, and the total timesteps are 45000. The training metrics are: approximate KL divergence of 0.006825491, clip fraction of 0.0723, clip range of 0.2, entropy loss of -2.8, explained variance of -0.106, learning rate of 0.0001, loss of -0.00205, 140 updates, policy gradient loss of -0.00912, standard deviation of 0.982, and value loss of 0.00559.
After 16 iterations and 90 seconds elapsed, the mean episode length is 1000, the mean episode reward is 9.92, the FPS is 532, and the total timesteps are 48000. The training metrics are: approximate KL divergence of 0.0047979683, clip fraction of 0.0477, clip range of 0.2, entropy loss of -2.8, explained variance of 0.179, learning rate of 0.0001, loss of -0.00573, 150 updates, policy gradient loss of -0.00603, standard deviation of 0.98, and value loss of 0.00393.
After 17 iterations and 96 seconds elapsed, the mean episode length is 1000, the mean episode reward is 10.0, the FPS is 533, and the total timesteps are 51000. The training metrics are: approximate KL divergence of 0.005295385, clip fraction of 0.0576, clip range of 0.2, entropy loss of -2.8, explained variance of -0.0362, learning rate of 0.0001, loss of -0.0145, 160 updates, policy gradient loss of -0.00653, standard deviation of 0.98, and value loss of 0.00446.
After 18 iterations and 101 seconds elapsed, the mean episode length is 1000, the mean episode reward is 10.1, the FPS is 534, and the total timesteps are 54000. The training metrics are: approximate KL divergence of 0.004252794, clip fraction of 0.0437, clip range of 0.2, entropy loss of -2.8, explained variance of -0.115, learning rate of 0.0001, loss of -0.0121, 170 updates, policy gradient loss of -0.00558, standard deviation of 0.978, and value loss of 0.00669.
After 19 iterations and 107 seconds elapsed, the mean episode length is 1000, the mean episode reward is 10.2, the FPS is 534, and the total timesteps are 57000. The training metrics are: approximate KL divergence of 0.0049305953, clip fraction of 0.0536, clip range of 0.2, entropy loss of -2.8, explained variance of -0.0731, learning rate of 0.0001, loss of -0.00459, 180 updates, policy gradient loss of -0.00755, standard deviation of 0.976, and value loss of 0.00445.
After 20 iterations and 113 seconds elapsed, the mean episode length is 1000, the mean episode reward is 10.3, the FPS is 534, and the total timesteps are 60000. The training metrics are: approximate KL divergence of 0.0058751277, clip fraction of 0.0683, clip range of 0.2, entropy loss of -2.8, explained variance of -0.105, learning rate of 0.0001, loss of -0.013, 190 updates, policy gradient loss of -0.00856, standard deviation of 0.976, and value loss of 0.00539

[Screencast from 07-02-2024 07:37:36 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/47065cad-b1dd-4ae7-817d-845af2a47d44)


The returns from 25 episodes show varying levels of performance. The highest return was achieved in Episode 18 with a value of 17.906, while the lowest was in Episode 15 with a return of 9.078. Several episodes consistently showed returns above 13, including Episodes 1, 3, 5, 7, 13, 14, and 20, with Episode 7 being notably high at 13.813. Some episodes such as 12, 15, 19, 23, and 24 had returns below 10, indicating variability in performance. Overall, the returns ranged between 9.078 and 17.906, with a mixture of high and low performing episodes throughout the 25 trials.


![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/101d7b85-5276-4409-ac47-dec93c16b008)

