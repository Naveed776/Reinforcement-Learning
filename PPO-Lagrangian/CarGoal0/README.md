# CarGoal0


After 6 seconds and 1 iteration, the model achieved a frame rate of 466 FPS with a total of 3000 timesteps.

By the 2nd iteration, 15 seconds had elapsed, resulting in 395 FPS and 6000 total timesteps. Training metrics were as follows: approx_kl was 0.0050040293, clip_fraction was 0.0433, clip_range was 0.2, entropy_loss was -2.84, explained_variance was -7.55, learning_rate was 0.0002, loss was -0.00822, n_updates were 10, policy_gradient_loss was -0.00421, std was 1.01, and value_loss was 0.00632.

In the 3rd iteration after 22 seconds, the FPS was 392 with 9000 total timesteps. Training metrics included: approx_kl of 0.007902339, clip_fraction of 0.0829, clip_range of 0.2, entropy_loss of -2.84, explained_variance of 0.11, learning_rate of 0.0002, loss of -0.0178, n_updates of 20, policy_gradient_loss of -0.00951, std of 0.999, and value_loss of 0.00385.

After 30 seconds and 4 iterations, the FPS was 389 with a total of 12000 timesteps. The training metrics were: approx_kl was 0.008181086, clip_fraction was 0.0767, clip_range was 0.2, entropy_loss was -2.84, explained_variance was 0.311, learning_rate was 0.0002, loss was -0.00793, n_updates were 30, policy_gradient_loss was -0.00812, std was 1, and value_loss was 0.00332.

By the 5th iteration, 38 seconds had elapsed, yielding 388 FPS and 15000 total timesteps. Training metrics were: approx_kl was 0.011152441, clip_fraction was 0.135, clip_range was 0.2, entropy_loss was -2.83, explained_variance was 0.255, learning_rate was 0.0002, loss was 0.000687, n_updates were 40, policy_gradient_loss was -0.0117, std was 0.994, and value_loss was 0.00346.

After 46 seconds and 6 iterations, the FPS was 389 with 18000 total timesteps. Training metrics included: approx_kl of 0.013431942, clip_fraction of 0.153, clip_range of 0.2, entropy_loss of -2.83, explained_variance of 0.0376, learning_rate of 0.0002, loss of -0.0123, n_updates of 50, policy_gradient_loss of -0.0109, std of 0.995, and value_loss of 0.00505.

By the 7th iteration, 54 seconds had passed, resulting in 388 FPS and 21000 total timesteps. The training metrics were: approx_kl was 0.009070957, clip_fraction was 0.111, clip_range was 0.2, entropy_loss was -2.81, explained_variance was -0.343, learning_rate was 0.0002, loss was 0.0107, n_updates were 60, policy_gradient_loss was -0.00838, std was 0.98, and value_loss was 0.00608.

After 61 seconds and 8 iterations, the FPS was 388 with 24000 total timesteps. Training metrics included: approx_kl of 0.009768481, clip_fraction of 0.132, clip_range of 0.2, entropy_loss of -2.8, explained_variance of 0.0523, learning_rate of 0.0002, loss of -0.0186, n_updates of 70, policy_gradient_loss of -0.00874, std of 0.983, and value_loss of 0.00756.

By the 9th iteration, 69 seconds had elapsed, achieving 386 FPS with 27000 total timesteps. Training metrics were: approx_kl was 0.0133874575, clip_fraction was 0.187, clip_range was 0.2, entropy_loss was -2.81, explained_variance was 0.114, learning_rate was 0.0002, loss was -0.0268, n_updates were 80, policy_gradient_loss was -0.0127, std was 0.984, and value_loss was 0.0106.

After 78 seconds and 10 iterations, the FPS was 384 with 30000 total timesteps. Training metrics included: approx_kl of 0.010736624, clip_fraction of 0.132, clip_range of 0.2, entropy_loss of -2.81, explained_variance of -0.0011, learning_rate of 0.0002, loss of -0.0172, n_updates of 90, policy_gradient_loss of -0.00862, std of 0.985, and value_loss of 0.014.

In the 11th iteration after 85 seconds, the FPS was 384 with 33000 total timesteps. Training metrics were: approx_kl was 0.0140347, clip_fraction was 0.14, clip_range was 0.2, entropy_loss was -2.82, explained_variance was -0.134, learning_rate was 0.0002, loss was 0.0582, n_updates were 100, policy_gradient_loss was -0.00732, std was 0.995, and value_loss was 0.017.

By the 12th iteration, 93 seconds had passed, resulting in 383 FPS and 36000 total timesteps. The training metrics included: approx_kl of 0.008352002, clip_fraction of 0.107, clip_range of 0.2, entropy_loss of -2.83, explained_variance of -0.0357, learning_rate of 0.0002, loss of -0.0136, n_updates of 110, policy_gradient_loss of -0.00644, std of 0.999, and value_loss of 0.0276.

After 101 seconds and 13 iterations, the FPS was 382 with 39000 total timesteps. Training metrics were: approx_kl was 0.008372323, clip_fraction was 0.0919, clip_range was 0.2, entropy_loss was -2.84, explained_variance was 0.137, learning_rate was 0.0002, loss was 0.00546, n_updates were 120, policy_gradient_loss was -0.00639, std was 0.999, and value_loss was 0.0216.

In the 14th iteration after 109 seconds, the FPS was 382 with 42000 total timesteps. Training metrics included: approx_kl of 0.01053756, clip_fraction of 0.128, clip_range of 0.2, entropy_loss of -2.83, explained_variance of -0.158, learning_rate of 0.0002, loss of 0.0309, n_updates of 130, policy_gradient_loss of -0.00911, std of 0.996, and value_loss of 0.027.

By the 15th iteration, 117 seconds had passed, yielding 381 FPS and 45000 total timesteps. The training metrics were: approx_kl was 0.011230865, clip_fraction was 0.106, clip_range was 0.2, entropy_loss was -2.83, explained_variance was 0.0976, learning_rate was 0.0002, loss was 0.0203, n_updates were 140, policy_gradient_loss was -0.0063, std was 0.996, and value_loss was 0.0265.

After 125 seconds and 16 iterations, the FPS was 382 with 48000 total timesteps. Training metrics included: approx_kl of 0.0072818263, clip_fraction of 0.109, clip_range of 0.2, entropy_loss of -2.83, explained_variance of 0.0759, learning_rate of 0.0002, loss of -0.00198, n_updates of 150, policy_gradient_loss of -0.00533, std of 0.996, and value_loss of 0.0345.

By the 17th iteration, 133 seconds had elapsed, achieving 381 FPS with 51000 total timesteps. Training metrics were: approx_kl was 0.010567943, clip_fraction was 0.13, clip_range was 0.2, entropy_loss was -2.83, explained_variance was 0.141, learning_rate was 0.0002, loss was 0.000834, n_updates were 160, policy_gradient_loss was -0.00918, std was 0.998, and value_loss was 0.0332.

After 141 seconds and 18 iterations, the FPS was 381 with 54000 total timesteps. Training metrics included: approx_kl of 0.009238922, clip_fraction of 0.105

, clip_range of 0.2, entropy_loss of -2.83, explained_variance of 0.0684, learning_rate of 0.0002, loss of -0.0126, n_updates of 170, policy_gradient_loss of -0.00568, std of 0.998, and value_loss of 0.0375.

In the 19th iteration after 149 seconds, the FPS was 381 with 57000 total timesteps. The training metrics were: approx_kl was 0.011729388, clip_fraction was 0.114, clip_range was 0.2, entropy_loss was -2.83, explained_variance was -0.123, learning_rate was 0.0002, loss was -0.00866, n_updates were 180, policy_gradient_loss was -0.00647, std was 0.998, and value_loss was 0.0413.

By the 20th iteration, 157 seconds had passed, resulting in 381 FPS and 60000 total timesteps. Training metrics included: approx_kl of 0.010135678, clip_fraction of 0.11, clip_range of 0.2, entropy_loss of -2.83, explained_variance of 0.0436, learning_rate of 0.0002, loss of -0.017, n_updates of 190, policy_gradient_loss of -0.006, std of 0.998, and value_loss of 0.0368.

[Screencast from 07-01-2024 08:09:58 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/64956868-cdcb-4666-9589-b7035762daf7)




The returns ranged from -0.133 to 8.254.
Episode 6 had the highest return (8.254).
Episode 18 had the lowest return (-0.133).
The average return was approximately 4.64.
No episode had any associated costs.


![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/62c3d891-0b0f-4366-a251-ba90182d8b54)

