# CarButton0

At 7 seconds, the frames per second (fps) was 417, with 1 iteration and a total of 3000 timesteps.

At 16 seconds, the fps was 364, with 2 iterations and a total of 6000 timesteps. Training data: approx_kl of 0.0094310595, clip_fraction of 0.102, clip_range of 0.2, entropy_loss of -2.83, explained_variance of -0.769, learning_rate of 0.0002, loss of -0.0225, n_updates of 10, policy_gradient_loss of -0.0119, std of 0.991, and value_loss of 0.0119.

At 24 seconds, the fps was 365, with 3 iterations and a total of 9000 timesteps. Training data: approx_kl of 0.008088047, clip_fraction of 0.0899, clip_range of 0.2, entropy_loss of -2.81, explained_variance of 0.115, learning_rate of 0.0002, loss of -0.00995, n_updates of 20, policy_gradient_loss of -0.00769, std of 0.981, and value_loss of 0.00616.

At 33 seconds, the fps was 359, with 4 iterations and a total of 12000 timesteps. Training data: approx_kl of 0.0102095865, clip_fraction of 0.116, clip_range of 0.2, entropy_loss of -2.79, explained_variance of 0.407, learning_rate of 0.0002, loss of 0.00974, n_updates of 30, policy_gradient_loss of -0.00949, std of 0.97, and value_loss of 0.0115.

At 41 seconds, the fps was 357, with 5 iterations and a total of 15000 timesteps. Training data: approx_kl of 0.010572285, clip_fraction of 0.127, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.523, learning_rate of 0.0002, loss of -0.0333, n_updates of 40, policy_gradient_loss of -0.00999, std of 0.968, and value_loss of 0.0134.

At 50 seconds, the fps was 359, with 6 iterations and a total of 18000 timesteps. Training data: approx_kl of 0.012190013, clip_fraction of 0.117, clip_range of 0.2, entropy_loss of -2.76, explained_variance of 0.487, learning_rate of 0.0002, loss of 0.0114, n_updates of 50, policy_gradient_loss of -0.01, std of 0.956, and value_loss of 0.02.

At 59 seconds, the fps was 355, with 7 iterations and a total of 21000 timesteps. Training data: approx_kl of 0.008963084, clip_fraction of 0.109, clip_range of 0.2, entropy_loss of -2.76, explained_variance of 0.709, learning_rate of 0.0002, loss of 0.0171, n_updates of 60, policy_gradient_loss of -0.0127, std of 0.966, and value_loss of 0.0202.

At 67 seconds, the fps was 356, with 8 iterations and a total of 24000 timesteps. Training data: approx_kl of 0.007508947, clip_fraction of 0.104, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.522, learning_rate of 0.0002, loss of 0.0112, n_updates of 70, policy_gradient_loss of -0.00862, std of 0.966, and value_loss of 0.0214.

At 76 seconds, the fps was 354, with 9 iterations and a total of 27000 timesteps. Training data: approx_kl of 0.009888575, clip_fraction of 0.117, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.528, learning_rate of 0.0002, loss of -0.00941, n_updates of 80, policy_gradient_loss of -0.0114, std of 0.966, and value_loss of 0.0199.

At 84 seconds, the fps was 356, with 10 iterations and a total of 30000 timesteps. Training data: approx_kl of 0.0079384465, clip_fraction of 0.0985, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.425, learning_rate of 0.0002, loss of 0.00795, n_updates of 90, policy_gradient_loss of -0.00708, std of 0.965, and value_loss of 0.0423.

At 91 seconds, the fps was 360, with 11 iterations and a total of 33000 timesteps. Training data: approx_kl of 0.01075857, clip_fraction of 0.145, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.177, learning_rate of 0.0002, loss of -0.0265, n_updates of 100, policy_gradient_loss of -0.0129, std of 0.966, and value_loss of 0.0358.

At 99 seconds, the fps was 361, with 12 iterations and a total of 36000 timesteps. Training data: approx_kl of 0.009371715, clip_fraction of 0.108, clip_range of 0.2, entropy_loss of -2.78, explained_variance of 0.291, learning_rate of 0.0002, loss of -0.00813, n_updates of 110, policy_gradient_loss of -0.0103, std of 0.972, and value_loss of 0.0307.

At 107 seconds, the fps was 363, with 13 iterations and a total of 39000 timesteps. Training data: approx_kl of 0.010789338, clip_fraction of 0.118, clip_range of 0.2, entropy_loss of -2.79, explained_variance of 0.635, learning_rate of 0.0002, loss of -0.0108, n_updates of 120, policy_gradient_loss of -0.0118, std of 0.975, and value_loss of 0.0307.

At 115 seconds, the fps was 363, with 14 iterations and a total of 42000 timesteps. Training data: approx_kl of 0.013553915, clip_fraction of 0.128, clip_range of 0.2, entropy_loss of -2.78, explained_variance of 0.524, learning_rate of 0.0002, loss of -0.00123, n_updates of 130, policy_gradient_loss of -0.0127, std of 0.966, and value_loss of 0.0576.

At 123 seconds, the fps was 364, with 15 iterations and a total of 45000 timesteps. Training data: approx_kl of 0.012463698, clip_fraction of 0.135, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.399, learning_rate of 0.0002, loss of 0.0323, n_updates of 140, policy_gradient_loss of -0.0126, std of 0.968, and value_loss of 0.0704.

At 131 seconds, the fps was 366, with 16 iterations and a total of 48000 timesteps. Training data: approx_kl of 0.009051221, clip_fraction of 0.114, clip_range of 0.2, entropy_loss of -2.77, explained_variance of 0.459, learning_rate of 0.0002, loss of -0.0118, n_updates of 150, policy_gradient_loss of -0.013, std of 0.963, and value_loss of 0.0836.

At 139 seconds, the fps was 366, with 17 iterations and a total of 51000 timesteps. Training data: approx_kl of 0.010091343, clip_fraction of 0.111, clip_range of 0.2, entropy_loss of -2.75, explained_variance of 0.367, learning_rate of 0.0002, loss of 0.0194, n_updates of 160, policy_gradient_loss of -0.011, std of 0.948, and value_loss of 0.0699.

[Screencast from 07-01-2024 08:22:45 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/5e147997-8029-4ca6-b02f-9e2151e66897)

Average Return: Approximately 9.07
Minimum Return: 1.476 (Episode 20)
Maximum Return: 13.758 (Episode 8)
Range of Returns: 12.282 (from 1.476 to 13.758)
These statistics give a quick overview of the performance across the 25 episodes, focusing on the range and central tendency of the returns observed.
![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/abc1738e-e6c9-4fbb-a054-a1469688cf6d)
