# PointGoal0

1. After the first iteration, the system achieved a frame rate of 503 FPS, with a total of 3,000 timesteps completed in 5 seconds.

2. In the second iteration, the FPS dropped to 451, accumulating 6,000 timesteps in 13 seconds. The training metrics included an approximate KL of 0.0038, clip fraction of 0.0437, clip range of 0.2, entropy loss of -2.84, explained variance of -3.51, learning rate of 0.0002, a loss of -0.00423, 10 updates, policy gradient loss of -0.00223, standard deviation of 0.999, and value loss of 0.00485.

3. By the third iteration, the FPS was 459 with a total of 9,000 timesteps in 19 seconds. Training metrics were: approximate KL of 0.0106, clip fraction of 0.085, clip range of 0.2, entropy loss of -2.84, explained variance of 0.138, learning rate of 0.0002, a loss of 0.00494, 20 updates, policy gradient loss of -0.00927, standard deviation of 1, and value loss of 0.00261.

4. During the fourth iteration, the FPS remained at 459, reaching 12,000 timesteps in 26 seconds. The training metrics included an approximate KL of 0.0101, clip fraction of 0.139, clip range of 0.2, entropy loss of -2.84, explained variance of 0.231, learning rate of 0.0002, a loss of -0.0295, 30 updates, policy gradient loss of -0.0107, standard deviation of 1, and value loss of 0.0019.

5. In the fifth iteration, the FPS was 464, with 15,000 timesteps achieved in 32 seconds. The training metrics were: approximate KL of 0.0091, clip fraction of 0.0958, clip range of 0.2, entropy loss of -2.82, explained variance of 0.00851, learning rate of 0.0002, a loss of 0.00568, 40 updates, policy gradient loss of -0.00477, standard deviation of 0.977, and value loss of 0.00867.

6. The sixth iteration showed an FPS of 465, accumulating 18,000 timesteps in 38 seconds. Training metrics were: approximate KL of 0.0103, clip fraction of 0.124, clip range of 0.2, entropy loss of -2.78, explained variance of 0.0322, learning rate of 0.0002, a loss of -0.0185, 50 updates, policy gradient loss of -0.00539, standard deviation of 0.969, and value loss of 0.018.

7. By the seventh iteration, the FPS was 457 with a total of 21,000 timesteps in 45 seconds. Training metrics included: approximate KL of 0.01, clip fraction of 0.102, clip range of 0.2, entropy loss of -2.77, explained variance of 0.216, learning rate of 0.0002, a loss of 0.00307, 60 updates, policy gradient loss of -0.00473, standard deviation of 0.966, and value loss of 0.0181.

8. During the eighth iteration, the FPS was 453, reaching 24,000 timesteps in 52 seconds. Training metrics were: approximate KL of 0.0105, clip fraction of 0.123, clip range of 0.2, entropy loss of -2.77, explained variance of 0.412, learning rate of 0.0002, a loss of 0.00397, 70 updates, policy gradient loss of -0.00525, standard deviation of 0.966, and value loss of 0.0244.

9. In the ninth iteration, the FPS was 448, with 27,000 timesteps completed in 60 seconds. The training metrics included an approximate KL of 0.0101, clip fraction of 0.11, clip range of 0.2, entropy loss of -2.78, explained variance of 0.62, learning rate of 0.0002, a loss of -0.00544, 80 updates, policy gradient loss of -0.00419, standard deviation of 0.973, and value loss of 0.0279.

10. By the tenth iteration, the FPS was 445, achieving 30,000 timesteps in 67 seconds. Training metrics were: approximate KL of 0.0117, clip fraction of 0.128, clip range of 0.2, entropy loss of -2.77, explained variance of 0.537, learning rate of 0.0002, a loss of 0.0104, 90 updates, policy gradient loss of -0.00789, standard deviation of 0.965, and value loss of 0.0364.

11. The eleventh iteration showed an FPS of 444, with 33,000 timesteps in 74 seconds. Training metrics included: approximate KL of 0.0111, clip fraction of 0.106, clip range of 0.2, entropy loss of -2.76, explained variance of 0.538, learning rate of 0.0002, a loss of 0.0182, 100 updates, policy gradient loss of -0.00264, standard deviation of 0.956, and value loss of 0.0509.

12. During the twelfth iteration, the FPS was 448, reaching 36,000 timesteps in 80 seconds. Training metrics were: approximate KL of 0.0115, clip fraction of 0.14, clip range of 0.2, entropy loss of -2.73, explained variance of 0.611, learning rate of 0.0002, a loss of 0.0739, 110 updates, policy gradient loss of -0.007, standard deviation of 0.944, and value loss of 0.0711.

13. By the thirteenth iteration, the FPS was 450, achieving 39,000 timesteps in 86 seconds. Training metrics included: approximate KL of 0.0068, clip fraction of 0.0878, clip range of 0.2, entropy loss of -2.71, explained variance of 0.484, learning rate of 0.0002, a loss of 0.0126, 120 updates, policy gradient loss of -0.00323, standard deviation of 0.933, and value loss of 0.0835.

14. In the fourteenth iteration, the FPS was 452, with a total of 42,000 timesteps in 92 seconds. Training metrics were: approximate KL of 0.0079, clip fraction of 0.108, clip range of 0.2, entropy loss of -2.7, explained variance of 0.574, learning rate of 0.0002, a loss of 0.0497, 130 updates, policy gradient loss of -0.00179, standard deviation of 0.934, and value loss of 0.0872.

15. The fifteenth iteration showed an FPS of 454, accumulating 45,000 timesteps in 99 seconds. Training metrics included: approximate KL of 0.0103, clip fraction of 0.124, clip range of 0.2, entropy loss of -2.69, explained variance of 0.336, learning rate of 0.0002, a loss of 0.0251, 140 updates, policy gradient loss of -0.00189, standard deviation of 0.921, and value loss of 0.133.

16. By the sixteenth iteration, the FPS was 455, with 48,000 timesteps in 105 seconds. Training metrics were: approximate KL of 0.0085, clip fraction of 0.126, clip range of 0.2, entropy loss of -2.68, explained variance of 0.237, learning rate of 0.0002, a loss of 0.149, 150 updates, policy gradient loss of -0.00242, standard deviation of 0.931, and value loss of 0.163.

17. During the seventeenth iteration, the FPS was 455, achieving 51,000 timesteps in 112 seconds. Training metrics included: approximate KL of 0.0079, clip fraction of 0.114, clip range of 0.2, entropy loss of -2.7, explained variance of 0.233, learning rate of 0.0002, a loss of 0.0717, 160 updates, policy gradient loss of -0.00131, standard deviation of 0.935, and value loss of 0.178.

18. In the eighteenth iteration, the FPS was 453, with 54,000 timesteps completed in 119 seconds. Training metrics were: approximate KL of 0.0118, clip fraction of 0.133, clip range of 0.2, entropy loss of -2.7, explained variance of 0.356, learning rate of 0.000

2, a loss of -0.0113, 170 updates, policy gradient loss of -0.00096, standard deviation of 0.936, and value loss of 0.179.

19. By the nineteenth iteration, the FPS was 450, reaching 57,000 timesteps in 125 seconds. Training metrics included: approximate KL of 0.0146, clip fraction of 0.136, clip range of 0.2, entropy loss of -2.7, explained variance of 0.157, learning rate of 0.0002, a loss of -0.0318, 180 updates, policy gradient loss of -0.00236, standard deviation of 0.941, and value loss of 0.19.

20. The twentieth iteration showed an FPS of 448, with 60,000 timesteps achieved in 132 seconds. Training metrics were: approximate KL of 0.0141, clip fraction of 0.133, clip range of 0.2, entropy loss of -2.69, explained variance of 0.288, learning rate of 0.0002, a loss of -0.0113, 190 updates, policy gradient loss of -0.00113, standard deviation of 0.943, and value loss of 0.219.


[Screencast from 07-01-2024 07:39:10 PM.webm](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/50fc29fa-c30c-4051-a7d8-796bb1a7a9f4)



# summarized version of the training process:

Iteration 1: 503 FPS, 3,000 timesteps in 5s.
Iteration 2: 451 FPS, 6,000 timesteps in 13s. Key metrics: KL 0.0038, loss -0.00423.
Iteration 3: 459 FPS, 9,000 timesteps in 19s. Key metrics: KL 0.0106, loss 0.00494.
Iteration 4: 459 FPS, 12,000 timesteps in 26s. Key metrics: KL 0.0101, loss -0.0295.
Iteration 5: 464 FPS, 15,000 timesteps in 32s. Key metrics: KL 0.0091, loss 0.00568.
Iteration 6: 465 FPS, 18,000 timesteps in 38s. Key metrics: KL 0.0103, loss -0.0185.
Iteration 7: 457 FPS, 21,000 timesteps in 45s. Key metrics: KL 0.01, loss 0.00307.
Iteration 8: 453 FPS, 24,000 timesteps in 52s. Key metrics: KL 0.0105, loss 0.00397.
Iteration 9: 448 FPS, 27,000 timesteps in 60s. Key metrics: KL 0.0101, loss -0.00544.
Iteration 10: 445 FPS, 30,000 timesteps in 67s. Key metrics: KL 0.0117, loss 0.0104.
Iteration 11: 444 FPS, 33,000 timesteps in 74s. Key metrics: KL 0.0111, loss 0.0182.
Iteration 12: 448 FPS, 36,000 timesteps in 80s. Key metrics: KL 0.0115, loss 0.0739.
Iteration 13: 450 FPS, 39,000 timesteps in 86s. Key metrics: KL 0.0068, loss 0.0126.
Iteration 14: 452 FPS, 42,000 timesteps in 92s. Key metrics: KL 0.0079, loss 0.0497.
Iteration 15: 454 FPS, 45,000 timesteps in 99s. Key metrics: KL 0.0103, loss 0.0251.
Iteration 16: 455 FPS, 48,000 timesteps in 105s. Key metrics: KL 0.0085, loss 0.149.
Iteration 17: 455 FPS, 51,000 timesteps in 112s. Key metrics: KL 0.0079, loss 0.0717.
Iteration 18: 453 FPS, 54,000 timesteps in 119s. Key metrics: KL 0.0118, loss -0.0113.
Iteration 19: 450 FPS, 57,000 timesteps in 125s. Key metrics: KL 0.0146, loss -0.0318.
Iteration 20: 448 FPS, 60,000 timesteps in 132s. Key metrics: KL 0.0141, loss -0.0113.
Overall, the training was stable with FPS ranging from 444 to 503 and varying key metrics such as approximate KL and loss.
# Episodes Return
The episodes had returns ranging from 18.902 to 23.394, with no episode incurring any cost. The returns generally fluctuated around the low 20s, with a few notable peaks and dips. The highest return was 23.394 in Episode 5, while the lowest was 18.902 in Episode 25. Overall, the performance was consistent, with minor variations throughout the series.
![Figure_1](https://github.com/Naveed776/Reinforcement-Learning/assets/91262613/bfdf8ca6-90e3-47e7-a16e-696d94faac2c)

